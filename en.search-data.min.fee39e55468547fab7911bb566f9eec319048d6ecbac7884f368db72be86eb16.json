[{"id":0,"href":"/posts/core-media-io-camera-extensions-part-one/","title":"Getting To Grips With The Core Media IO Camera Extension Part 1 of 3: The Basics","section":"Posts","content":"Getting To Grips With The Core Media IO Camera Extension, a 3 part series. #  Part 1 of 3: The Basics: creating an installable Core Media Camera Extension and its container app. #  August 11th, 2022 To the code â†’\n View project on Github #  Welcome to the first in a series of three posts about the Core Media IO Camera Extension. I was delighted by the Create camera extensions with Core Media IO  WWDC22 video, which is about extensions which can present a system camera for use in any camera-supporting app such as FaceTime, including creative cameras that can take in a feed from an existing camera such as a Continuity Camera Webcam and add effects to it. Still, I wished that it had sample code for the two types of cameras that it discussed, software camera and creative camera with configuration app.\nI wrote myself some sample code, and now I would like to share it, and explain a little about how it works.\nThere will be three posts in the series:\nThe first post, this one, is about getting the basics working from the start,\nthe second is about creating a useful software CMIO Camera Extension with communication between a configuration app and an extension and painless extension debugging,\nand the last is about bringing it all together by building a creative camera with realtime effects processing using vImage Pixel Buffers, which can use the Continuity Camera Webcam.\nEach will build on the previous post. SwiftUI will be the only interface framework used, and there will be no NSViewRepresentable, which is going to get spicy in the final entry when we\u0026rsquo;re observing a camera feed in the configuration app.\nPrerequisites #  Much of this will work in macOS 12.3 and later with Xcode 13, so my sample app for part 1 will build, run and work with them. But, the end result will explore beta APIs, so this series as a whole has been written for the betas. To run all of the code in all three parts:\n Ventura beta 5 or later Xcode 14 beta 5 or later An iPhone XS or later running iOS 16 beta 5 or later if you want to test a Continuity Camera Webcam camera extension in later parts of this series An Apple Developer account so you have a team ID and can codesign, which is already working correctly on your system when you sign other apps. If you\u0026rsquo;ve had some codesign weirdness you\u0026rsquo;ve been ignoring, this isn\u0026rsquo;t the project for working through it, trust me. It is certainly a good idea to have looked at the WWDC video and the docs, but it isn\u0026rsquo;t a requirement.  Let\u0026rsquo;s jump in. Here are the instructions for creating a container app and installable CMIO Camera Extension which is a basic software camera (you can also clone mine from Github or refer to it as you go):\nProject configuration #   Create a new project of type \u0026ldquo;macOS App\u0026rdquo;. Title it \u0026ldquo;OffcutsCam\u0026rdquo;, select your team and organization identifier. Your organization identifier will be different than mine.     Go to the app target, select Signing \u0026amp; Capabilities, and remove \u0026ldquo;Hardened Runtime\u0026rdquo;. Under App Sandbox, check \u0026ldquo;Camera\u0026rdquo; and, for debugging purposes only, set \u0026ldquo;User Selected File\u0026rdquo; to \u0026ldquo;Read Only\u0026rdquo;.     Click the \u0026ldquo;+\u0026rdquo; button at the top left to add a capability. Add \u0026ldquo;System Extension\u0026rdquo;.    Now add a target to your project (File-\u0026gt;New-\u0026gt;Target). Scroll all the way down in the target types to choose \u0026ldquo;Camera Extension\u0026rdquo;. Title it \u0026ldquo;Extension\u0026rdquo;     Go to the new extension target, select Signing \u0026amp; Capabilities, and remove \u0026ldquo;Hardened Runtime\u0026rdquo;. Under App Sandbox, check \u0026ldquo;Camera\u0026rdquo; and, for debugging purposes only, set \u0026ldquo;User Selected File\u0026rdquo; to \u0026ldquo;Read Only\u0026rdquo;.     See that the extension target has an App Group. Change the extension target app group to $(TeamIdentifierPrefix)com.politepix.OffcutsCam but use your organization identifier instead of mine (com.politepix). Copy this app group name.    Now go back to your app target, Signing \u0026amp; Capabilities, and add the capability \u0026ldquo;App Group\u0026rdquo;. In the new configuration area this adds, click \u0026ldquo;+\u0026rdquo; and add an app group with the identical app group name as the extension ($(TeamIdentifierPrefix)com.politepix.OffcutsCam (but with your organization identifier instead of mine). Both of these targets are allowed to communicate with each other via this App Group, and are now able to, and we will make use of this in the following two posts.    Still in your app target, go to Info and add the key Privacy - Camera Usage Description with a description of Camera Extension and while you\u0026rsquo;re here, just to avoid a distracting warning later, add the key App Category and set it to Utilities.     Switching to your extension target, go to Info and add the key Privacy - Camera Usage Description with a description of Camera Extension. There should be a key entitled Privacy - System Extension Usage Description (if there isn\u0026rsquo;t, create it). Add the description Camera Extension to it.     In the extension target, under Info, there should be a key CMIOExtension which is a dictionary. It should contain a key CMIOExtensionMachServiceName. The value of this key should be $(TeamIdentifierPrefix)$(PRODUCT_BUNDLE_IDENTIFIER). Change it to $(TeamIdentifierPrefix)com.politepix.OffcutsCam (but with your organization identifier instead of mine).    Now, theoretically, doing these steps in this order should result in a properly-configured Container App and Embedded System Extension, where the container app is allowed to install its embedded system extension into macOS. To verify this, you can check the following things in your entitlements files:\nApp entitlements should have a System Extension key set to YES. It should have an App Groups array with the first element the string $(TeamIdentifierPrefix)com.politepix.OffcutsCam but with your organization identifier instead of mine. It should have a key Camera set to YES. Here is a screenshot:\n  Extension entitlements should have an identical App Groups array. It should have a key Camera set to YES. Here is a screenshot:\n  The app target\u0026rsquo;s General pane should show the extension as embedded \u0026ldquo;(Embed Without Signing)\u0026rdquo; under Frameworks, Libraries, and Embedded Content.\n  If any of these aren\u0026rsquo;t right, review and see if you set things up correctly. You can compare to my completed version.\nIf this looks good, you can build and run. You should see a \u0026ldquo;Hello, world!\u0026rdquo; app. You can quit it. Go to /Applications/OffcutsCam.app and right-click and choose Show Package Contents the package and verify that you can see the extension inside of it like in this screenshot.\n  OK, stretch your legs for a moment and we\u0026rsquo;ll start configuring the extension and app source.\nReady?\nSource configuration #    Open ExtensionProvider.swift in the editor. This is where the input and output streams for the camera extension are managed. Apple is kind enough to provide a 100% known-working software camera in all fresh ExtensionProviders. I love that they do this.\n  Do a search and replace in this file for every occurrence of SampleCapture and change it to OffcutsCam. Change the occurrence of OffcutsCam (Swift) to just OffcutsCam. This is how we are letting the system and the user know which camera this is. That\u0026rsquo;s it for ExtensionProvider.swift, for this post (there will be much more ExtensionProvider in the following two posts). But for now, we want to get this default software camera extension fully working so we have a clean canvas to paint on.\n  Next, we will use Brad Ford\u0026rsquo;s onscreen sample code for extension install and uninstall from Create camera extensions with Core Media IO , adding it to our container app. First, add import SystemExtensions at the top of the App\u0026rsquo;s file ContentView.swift.\n  Then, add the following class to ContentView.swift (my only addition here was more verbose error logging):\nclass SystemExtensionRequestManager: NSObject, ObservableObject { @Published var logText: String = \u0026#34;Installation results here\u0026#34; init(logText: String) { super.init() self.logText = logText } func install() { guard let extensionIdentifier = _extensionBundle().bundleIdentifier else { return } let activationRequest = OSSystemExtensionRequest.activationRequest(forExtensionWithIdentifier: extensionIdentifier, queue: .main) activationRequest.delegate = self OSSystemExtensionManager.shared.submitRequest(activationRequest) } func uninstall() { guard let extensionIdentifier = _extensionBundle().bundleIdentifier else { return } let deactivationRequest = OSSystemExtensionRequest.deactivationRequest(forExtensionWithIdentifier: extensionIdentifier, queue: .main) deactivationRequest.delegate = self OSSystemExtensionManager.shared.submitRequest(deactivationRequest) } func _extensionBundle() -\u0026gt; Bundle { let extensionsDirectoryURL = URL(fileURLWithPath: \u0026#34;Contents/Library/SystemExtensions\u0026#34;, relativeTo: Bundle.main.bundleURL) let extensionURLs: [URL] do { extensionURLs = try FileManager.default.contentsOfDirectory(at: extensionsDirectoryURL, includingPropertiesForKeys: nil, options: .skipsHiddenFiles) } catch { fatalError(\u0026#34;failed to get the contents of \\(extensionsDirectoryURL.absoluteString): \\(error.localizedDescription)\u0026#34;) } guard let extensionURL = extensionURLs.first else { fatalError(\u0026#34;failed to find any system extensions\u0026#34;) } guard let extensionBundle = Bundle(url: extensionURL) else { fatalError(\u0026#34;failed to create a bundle with URL \\(extensionURL.absoluteString)\u0026#34;) } return extensionBundle } } extension SystemExtensionRequestManager: OSSystemExtensionRequestDelegate { public func request(_ request: OSSystemExtensionRequest, actionForReplacingExtension existing: OSSystemExtensionProperties, withExtension ext: OSSystemExtensionProperties) -\u0026gt; OSSystemExtensionRequest.ReplacementAction { logText = \u0026#34;Replacing extension version \\(existing.bundleShortVersion) with \\(ext.bundleShortVersion)\u0026#34; return .replace } public func requestNeedsUserApproval(_ request: OSSystemExtensionRequest) { logText = \u0026#34;Extension needs user approval\u0026#34; } public func request(_ request: OSSystemExtensionRequest, didFinishWithResult result: OSSystemExtensionRequest.Result) { switch result.rawValue { case 0: logText = \u0026#34;\\(request) did finish with success\u0026#34; case 1: logText = \u0026#34;\\(request) Extension did finish with result success but requires reboot\u0026#34; default: logText = \u0026#34;\\(request) Extension did finish with result \\(result)\u0026#34; } } public func request(_ request: OSSystemExtensionRequest, didFailWithError error: Error) { let errorCode = (error as NSError).code var errorString = \u0026#34;\u0026#34; switch errorCode { case 1: errorString = \u0026#34;unknown error\u0026#34; case 2: errorString = \u0026#34;missing entitlement\u0026#34; case 3: errorString = \u0026#34;Container App for Extension has to be in /Applications to install Extension.\u0026#34; case 4: errorString = \u0026#34;extension not found\u0026#34; case 5: errorString = \u0026#34;extension missing identifier\u0026#34; case 6: errorString = \u0026#34;duplicate extension identifer\u0026#34; case 7: errorString = \u0026#34;unknown extension category\u0026#34; case 8: errorString = \u0026#34;code signature invalid\u0026#34; case 9: errorString = \u0026#34;validation failed\u0026#34; case 10: errorString = \u0026#34;forbidden by system policy\u0026#34; case 11: errorString = \u0026#34;request canceled\u0026#34; case 12: errorString = \u0026#34;request superseded\u0026#34; case 13: errorString = \u0026#34;authorization required\u0026#34; default: errorString = \u0026#34;unknown code\u0026#34; } logText = \u0026#34;Extension did fail with error: \\(errorString)\u0026#34; } } in your ContentView, add this var:  @ObservedObject var systemExtensionRequestManager: SystemExtensionRequestManager\nand this view content:\n VStack { Button(\u0026#34;Install\u0026#34;, action: { systemExtensionRequestManager.install() }) Button(\u0026#34;Uninstall\u0026#34;, action: { systemExtensionRequestManager.uninstall() }) } Text(systemExtensionRequestManager.logText) } Change the calls in the container app that load ContentView to load it with its new SystemExtensionRequestManager and let\u0026rsquo;s give the frame a minimum size:\nContentView(systemExtensionRequestManager: SystemExtensionRequestManager(logText: \u0026#34;Starting container app\u0026#34;)) .frame(minWidth: 300, minHeight: 180) Now build and run the app again. You should see a UI with an Install and an Uninstall button.\nClick \u0026ldquo;Install\u0026rdquo; and see that you get an informative error that the container app isn\u0026rsquo;t in /Applications. System extensions have to be installed from a container app in /Applications to be acceptable to the system.\nLet\u0026rsquo;s make things easy on ourselves and set things up so the builds are moved to /Applications so we can install the codesigned extension like an enduser would, but without too much bother. Go to the app target\u0026rsquo;s Build Phases tab and click \u0026ldquo;+\u0026rdquo; and add a new Copy Files phase. For \u0026ldquo;Destination\u0026rdquo; choose \u0026ldquo;Absolute Path\u0026rdquo; and enter /Applications for the path. Under \u0026ldquo;Add files here\u0026rdquo; click \u0026ldquo;+\u0026rdquo; and select OffcutsCam.app from the \u0026ldquo;Products\u0026rdquo; section. Now a copy of the debug build will always be copied into /Applications. Build now, and verify that OffcutsCam.app is indeed in /Applications.\n  Next, edit the scheme of OffcutsCam (not the extension). Under Run-\u0026gt;Info change the Executable to \u0026ldquo;Other\u0026rdquo; and then select OffcutsCam.app in /Applications. This way, when Xcode lldb attaches to your build, it will attach to the build in /Applications, so you are only debugging the build that is being moved to /Applications.\n  With these steps complete, if you build and run OffcutsCam.app (we never need to run the extension directly), you should be able to click \u0026ldquo;Install\u0026rdquo; in the app UI and do an installation of your extension. The app should log that the extension needs user approval and macOS should show an alert saying \u0026ldquo;System Extension Blocked\u0026rdquo;. That\u0026rsquo;s great! That\u0026rsquo;s how it\u0026rsquo;s supposed to work.\n   Click \u0026ldquo;Open System Settings\u0026rdquo; on that alert (if you clicked \u0026ldquo;OK\u0026rdquo; instead that\u0026rsquo;s fine, go ahead and open the System Settings.app section \u0026ldquo;Security \u0026amp; Privacy\u0026rdquo;). Scroll down until you see the notification System software from application \u0026quot;OffcutsCam\u0026quot; was prevented from loading and click Allow.\n  Authenticate to install. Now the extension should be installed in your system. You can verify this by opening FaceTime. Under the Video menu you should see OffcutsCam as an offered camera. If you select it, you should see a black screen with a white line moving up and down it. Congrats! Your first CMIO Camera Extension.\nWe have taken pains to get codesigned extension installation working from the start so that we don\u0026rsquo;t need to debug this area of the project while debugging other complexities later on such as interprocess communication and realtime video processing.\nNote: if you start getting a codesigning error that the extension doesn\u0026rsquo;t match the app when building or running, or similar intermittent complaining at build that doesn\u0026rsquo;t seem 100% believable, this can be fixed by navigating to the app\u0026rsquo;s Build Phases, and under your \u0026ldquo;Copy Files\u0026rdquo; phase that moves the executable to /Applications, checking \u0026ldquo;Copy only when installing\u0026rdquo;, building and running, and then unchecking it again and building and running. This fragility is not great, but I think it\u0026rsquo;s preferable to building and running in DerivedData or turning off SIP in order to disable the codesigning requirements.\nAnd that brings us to the conclusion of part one of this series: we have created a Container App which can install and uninstall a working Core Media IO Camera System Extension that can be selected in FaceTime, and we have removed one big pain point already, which is testing this behavior with full codesigning while still being able to do a normal build and run.\nMy working version can be seen here, so if you are getting weird results, you can compare them.\nOnce you have things working, if you want to play with the extension provider code (and if you like video code, you probably do), be aware that to see software changes in your extension, you currently have to uninstall the old extension using your container app, then reboot, then build and run your new container app iteration, then install the changed extension. This is also currently the case if you follow the debugging advice for loading a new extension iteration without reversioning it (i.e., turning off SIP and turning on developer mode will not help with this). If I understand correctly, there is no way currently to securely replace the extension process in a single user session in which there has already been an extension install and activation, so a reboot is necessary.\nThis reboot-a-rama is the pain point we are going to remove next, in Core Media IO Camera Extensions part 2 of 3: Creating a useful software CMIO Camera Extension with communication between a configuration app and an extension and painless extension debugging, out shortly.\nExtro #  âœ‚ - - Non-code thoughts, please stop reading here for the code-only experience - - âœ‚\nSince we\u0026rsquo;re combining system extensions, video code, and codesigning, i.e. discussing pain, I thought it might be a good opportunity for some stoic philosophy.\nThere were three big stoics whose work we know about; Seneca, Epictetus and Marcus Aurelius. I\u0026rsquo;m not crazy about Seneca. Marcus Aurelius had some very helpful things to say, but he was an extraordinarily powerful man who has become a sort of aspirational lifestyle stoic: philosopher most likely to be cited in a LinkedIn post. For me, Epictetus is the most interesting. An ancient Greek philosopher, a onetime slave, someone who had to avoid anti-philosopher purges now and again â€“ you\u0026rsquo;re reading a blog post about camera extensions so it\u0026rsquo;s pretty likely that you and I are in the same lucky club of people who have no conception of the perils and problems he experienced, and it\u0026rsquo;s pretty moving to be able to hear his thoughts on how to think about them millenia later.\nAnyway he had this one piece of advice from the Encheiridion (\u0026ldquo;The Handbook\u0026rdquo;) that I think of a lot when I look at our industry. Epictetus is talking about how people jockey to do favors for the powerful, and the way it helps them rise in social or academic status, and that you, philosopher he is addressing, also want this status, but without all the ass-kissing, because it\u0026rsquo;s immoral. And he says (slightly modernized from W.A. Oldfather translation):\n\u0026ldquo;Well, what is the price of a head of lettuce? A dollar perhaps. If, then, somebody gives up his dollar and gets his head of lettuce, while you do not give your dollar, and do not get it, do not imagine that you are worse off than the man who gets his lettuce. For as he has his head of lettuce, so you have your dollar which you have not given away.\u0026rdquo;\nThere are a few points here. First of all, if you really believe this kind of status-seeking is immoral, then you haven\u0026rsquo;t lost anything, and it\u0026rsquo;s important to notice that â€“ it can be difficult to notice because there are a lot of signals from our society which say that status is more important than values. You still have your morality, which should have a very high value to you, so that is actually a good deal. If it still feels like a bad deal, you may have a different opinion of the morality of the situation, or morality in general, than you think you do, or (more likely, in my experience) you may have substituted someone else\u0026rsquo;s opinion for your own.\nSecond, IMO Epictetus is making a little joke by comparing status and morality to lettuce, and the amount of money that lettuce costs, respectively.\nThird, there\u0026rsquo;s an unspoken implication that either can still be chosen differently. Exchanging a dollar for lettuce is easier than exchanging lettuce for a dollar, granted â€“ that is part of his point. But, you could sell your lettuce for a dollar or return it for your original dollar. You could spend your dollar on lettuce. Because of this, his main point is that you shouldn\u0026rsquo;t complain, primarily if you have a dollar but no lettuce, which he thought was the better deal, but perhaps also if you have a lettuce and no dollar. Change the deal to the one you can live with, or accept your decision, but don\u0026rsquo;t lose your limited time envying people who have something you decided that you don\u0026rsquo;t want.\nIn the context of tech, I often think about this advice, not in terms of social status, which isn\u0026rsquo;t a giant influence in the life of software developers (although it is, of course, in the mix), but more in terms of the question of what problems we choose to use our skills to solve and for what compensations, and how we feel about our choices.\n"},{"id":1,"href":"/posts/prototyping-a-stationary-bike-stepper/","title":"Prototyping a stationary bike \"step\" counter with Swift Charts","section":"Posts","content":"Prototyping a stationary bike \u0026ldquo;step\u0026rdquo; counter with Swift Charts #  July 11th, 2022 To the code â†’\n I was recently chatting with a friend about SwiftUI, and I admitted that I had skipped the awkward first year entirely, but since I had gotten into it, it was reigniting my joy of the platform. \u0026ldquo;Why?\u0026rdquo; he asked. I had to think for a moment. \u0026ldquo;I love prototyping, and SwiftUI removed blockers that I had stopped noticing.\u0026rdquo;\nBack in the age of the dinosaurs, when I was doing a technical proof of concept, I preferred to write Objective-C UIs in code. There\u0026rsquo;s something very consonant, very nimble, about being able to take in an entire logical graph in a single zone of attention: maybe just one file you can see all of. When you\u0026rsquo;re just finding out if something is possible, the tradeoffs of that approach are compensated in flow state.\nSwiftUI feels the same but with more concision, and without the tradeoffs.\nIn the present, with a particular itch to scratch: I have a cheap stationary bike, perfect for my workouts in every respect except its inhumane display interface. And I spend so much time looking at the display and thinking about it! Should I run its signal wires into an MCU and make my own display? Should I use computer vision to consume its data? Is there something like Van Eck phreaking for LEDs?\nOr, can I forget about the display entirely, estimate the bike\u0026rsquo;s stationary km/h using sensors from my phone when it\u0026rsquo;s in the bike\u0026rsquo;s reading material holder, and calculate everything else that\u0026rsquo;s interesting?\nCan I? #  I know from working with a 9-DoF sensor in hardware projects that the odds are decent there\u0026rsquo;s some perceptible movement to use. But while I have theories about which sensors are likely to register the oscillation of a stationary bike (guesses: accelerometer and gyroscope), I wouldn\u0026rsquo;t assume: it\u0026rsquo;s even possible that pedaling the bike produces a field that could be picked up reliably on another sensor. No doubt any data will be small and noisy. I guess I need a prototype!\nThe iPhone\u0026rsquo;s 9-DoF sensors output readings which are arrays of doubles, but like all samples of electromechanical devices, such as the MEMs sensors in an iPhone, these samples are just pointillist representations of real waves. When we talk about extracting meaning from the samples, we use perceptual language like smoothing. If this works, we will most easily see it visually, so we should chart the sensors.\nSwift Charts #  When I first decided to do this post, I checked out all the chart packages for SwiftUI, but none of them quite spoke to me. I put the idea aside for a few days and decided to come back to it later and just pick one, and when I came back to it, WWDC had happened and Apple had released a really snazzy chart framework just for me, thanks Apple! [1]  (German has a highly-pejorative word for taking action purely for the sake of taking action, but it lacks a word for reaping the benefits of inaction.)   var footnotesubscript = document.getElementById(\"footnotesubscript_id_1\"); footnotesubscript.style.cursor = \"pointer\"; footnotesubscript.onclick = function () { var footnote = document.getElementById(\"footnote_id_1\"); if (footnote.style.visibility == \"visible\") { footnote.style.visibility = \"hidden\"; footnote.style.display = \"none\"; footnote.style.position = \"absolute\"; } else { footnote.style.visibility = \"visible\"; footnote.style.position = \"relative\"; footnote.style.display = \"inline\"; } };  This felt like more supporting data for my warm Swift prototyping feelings. It also means you can only run this code from an Xcode 14 beta or later to a real device running an iOS 16 beta or later.\nGoals: #  Write very minimal code for a single view with a chart of every sensor. Don\u0026rsquo;t extensively state- and error-handle; it should be run in portrait mode and it\u0026rsquo;s fine to assume a functioning device. Don\u0026rsquo;t worry about nice user interactions [2]  (I think there is a difference between a UI and a DI or developer interface, and like a wireframe and a design artwork, it can be a pretty good idea to prevent them from being confused with each other)   var footnotesubscript = document.getElementById(\"footnotesubscript_id_2\"); footnotesubscript.style.cursor = \"pointer\"; footnotesubscript.onclick = function () { var footnote = document.getElementById(\"footnote_id_2\"); if (footnote.style.visibility == \"visible\") { footnote.style.visibility = \"hidden\"; footnote.style.display = \"none\"; footnote.style.position = \"absolute\"; } else { footnote.style.visibility = \"visible\"; footnote.style.position = \"relative\"; footnote.style.display = \"inline\"; } }; . Make it possible to:\n see all the sensors, turn off the views of the sensors without relevant data, break out the three sensor axes to inspect them, and, for promising sensors, be able to smooth the wave and count the number of wave peaks (which are equivalent to pedaling \u0026ldquo;strides\u0026rdquo;).  The technical prototype is the question \u0026ldquo;is this possible?\u0026rdquo;; what\u0026rsquo;s inside it is the least complexity that would provide the answer. A product would present a second question, which is \u0026ldquo;can this be generalized?\u0026rdquo;, and the prudent answer is \u0026ldquo;not necessarily\u0026quot; [3]  (although I have some theories)   var footnotesubscript = document.getElementById(\"footnotesubscript_id_3\"); footnotesubscript.style.cursor = \"pointer\"; footnotesubscript.onclick = function () { var footnote = document.getElementById(\"footnote_id_3\"); if (footnote.style.visibility == \"visible\") { footnote.style.visibility = \"hidden\"; footnote.style.display = \"none\"; footnote.style.position = \"absolute\"; } else { footnote.style.visibility = \"visible\"; footnote.style.position = \"relative\"; footnote.style.display = \"inline\"; } }; .\nIn an app, I wouldn\u0026rsquo;t have a model and an interface in the same file. However, another thing I like about SwiftUI is the ability to give someone a single file they can drop into an app and try out just by initializing it in the App struct, with a UI and everything, wow. Trying things out is the name of the game here, so a single file will be my preference in this blog. But I added some fancy comment formatting so it\u0026rsquo;s easy to draw attention to interesting things from inside the file, and indicate when we\u0026rsquo;re in different kinds of modules.\nContentView.swift #  (view on Github)\nhljs.addPlugin(new CopyButtonPlugin()); hljs.highlightLinesAll([[ {start: '13', end: '13', color: 'yellow'}, {start: ' 37', end: '38', color: 'yellow'}, {start: ' 56', end: '56', color: 'yellow'}, {start: ' 79', end: '79', color: 'yellow'}, {start: ' 121', end: '126', color: 'yellow'}, {start: ' 161', end: '161', color: 'yellow'}, {start: ' 232', end: '235', color: 'yellow'}, ],]); // Created by Halle Winkler on July/11/22. Copyright Â© 2022. All rights reserved. // Requires Xcode 14.x and iOS 16.x, betas included. import Charts import CoreMotion import SwiftUI // MARK: - ContentView /// ContentView is a collection of motion sensor UIs and a method of calling back to the model. struct ContentView { @ObservedObject var manager: MotionManager } extension ContentView: View { var body: some View { VStack { ForEach(manager.sensors, id: \\.sensorName) { sensor in SensorChart(sensor: sensor) { applyFilter, lowPassFilterFactor, quantizeFactor in manager.updateFilteringFor( sensor: sensor, applyFilter: applyFilter, lowPassFilterFactor: lowPassFilterFactor, quantizeFactor: quantizeFactor) } } }.padding([.leading, .trailing], 6) } } // MARK: - SensorChart /// I like to compose SwiftUI interfaces out of many small modules. But, there is a tension when it's a /// small UI overall, and the modules will each have overhead from propagating state, binding and callbacks. struct SensorChart { @State private var chartIsVisible = true @State private var breakOutAxes = false @State private var applyingFilter = false @State private var lowPassFilterFactor: Double = 0.75 @State private var quantizeFactor: Double = 50 var sensor: Sensor let updateFiltering: (Bool, Double, Double) - Void private func toggleFiltering() { applyingFilter.toggle() updateFiltering(applyingFilter, lowPassFilterFactor, quantizeFactor) } } extension SensorChart: View { var body: some View { /// Per-sensor controls: apply filtering to the waveform, hide and show sensor, break out the axes into separate charts. HStack { Text(\"\\(sensor.sensorName)\") .font(.system(size: 12, weight: .semibold, design: .default)) .foregroundColor(chartIsVisible ? .black : .gray) Spacer() Button(action: toggleFiltering) { Image(systemName: applyingFilter ? \"waveform.circle.fill\" : \"waveform.circle\") } .opacity(chartIsVisible ? 1.0 : 0.0) Button(action: { chartIsVisible.toggle() }) { Image(systemName: chartIsVisible ? \"eye.circle.fill\" : \"eye.slash.circle\") } Button(action: { breakOutAxes.toggle() }) { Image(systemName: breakOutAxes ? \"1.circle.fill\" : \"3.circle.fill\") } .opacity(chartIsVisible ? 1.0 : 0.0) } /// Sensor charts, either one chart with three axes, or three charts with one axis. I love how concise Swift Charts can be. if chartIsVisible { if breakOutAxes { ForEach(sensor.axes, id: \\.axisName) { series in // Iterate charts from series Chart { ForEach( Array(series.measurements.enumerated()), id: \\.offset) { index, datum in LineMark( x: .value(\"Count\", index), y: .value(\"Measurement\", datum)) } } Text( \"Axis: \\(series.axisName)\\(applyingFilter ? \"\\t\\tPeaks in window: \\(series.peaks)\" : \"\")\") } .chartXAxis { AxisMarks(values: .automatic(desiredCount: 0)) } } else { Chart { ForEach(sensor.axes, id: \\.axisName) { series in // Iterate series in a chart ForEach( Array(series.measurements.enumerated()), id: \\.offset) { index, datum in LineMark( x: .value(\"Count\", index), y: .value(\"Measurement\", datum)) } .foregroundStyle(by: .value(\"MeasurementName\", series.axisName)) } }.chartXAxis { AxisMarks(values: .automatic(desiredCount: 0)) }.chartYAxis { AxisMarks(values: .automatic(desiredCount: 2)) } } /// in the separate three-axis view, you can set the low-pass filter factor and the quantizing factor if the waveform /// filtering is on, and then once you can see your stationary pedaling reflected in the waveform, you can see how /// many times per time window you're pedaling. With such an inevitably-noisy sensor environment, I already know /// the low-pass filter factor will have to be very high, so I'm starting it at 0.75. /// In the case of my exercise bike, the quantizing factor that delivers very accurate peak-counting results on /// gyroscope axis z is 520, which tells you these readings are really small numbers. if applyingFilter { Slider( value: $lowPassFilterFactor, in: 0.75 ... 1.0, onEditingChanged: { _ in updateFiltering( true, lowPassFilterFactor, quantizeFactor) }) Text(\"Lowpass: \\(String(format: \"%.2f\", lowPassFilterFactor))\") .font(.system(size: 12)) .frame(width: 100, alignment: .trailing) Slider( value: $quantizeFactor, in: 1 ... 600, onEditingChanged: { _ in updateFiltering( true, lowPassFilterFactor, quantizeFactor) }) Text(\"Quantize: \\(Int(quantizeFactor))\") .font(.system(size: 12)) .frame(width: 100, alignment: .trailing) } } Divider() } } // MARK: - MotionManager /// MotionManager is the sensor management module. class MotionManager: ObservableObject { // MARK: Lifecycle init() { self.manager = CMMotionManager() for name in SensorNames .allCases { // self.sensors and func collectReadings(...) use SensorNames to index, if name == .attitude { // so if you change how one creates/derives a sensor index, change them both. sensors.append(ThreeAxisReadings( sensorName: SensorNames.attitude.rawValue, // The one exception to sensor axis naming: axes: [ Axis(axisName: \"Pitch\"), Axis(axisName: \"Roll\"), Axis(axisName: \"Yaw\"), ])) } else { sensors.append(ThreeAxisReadings(sensorName: name.rawValue)) } } self.manager.deviceMotionUpdateInterval = sensorUpdateInterval self.manager.accelerometerUpdateInterval = sensorUpdateInterval self.manager.gyroUpdateInterval = sensorUpdateInterval self.manager.magnetometerUpdateInterval = sensorUpdateInterval self.startDeviceUpdates(manager: manager) } // MARK: Public public func updateFilteringFor( // Manage the callbacks from the UI sensor: ThreeAxisReadings, applyFilter: Bool, lowPassFilterFactor: Double, quantizeFactor: Double) { guard let index = sensors.firstIndex(of: sensor) else { return } DispatchQueue.main.async { self.sensors[index].applyFilter = applyFilter self.sensors[index].lowPassFilterFactor = lowPassFilterFactor self.sensors[index].quantizeFactor = quantizeFactor } } // MARK: Internal struct ThreeAxisReadings: Equatable { var sensorName: String // Usually, these have the same naming: var axes: [Axis] = [Axis(axisName: \"x\"), Axis(axisName: \"y\"), Axis(axisName: \"z\")] var applyFilter: Bool = false var lowPassFilterFactor = 0.75 var quantizeFactor = 1.0 func lowPassFilter(lastReading: Double?, newReading: Double) - Double { guard let lastReading else { return newReading } return self .lowPassFilterFactor * lastReading + (1.0 - self.lowPassFilterFactor) * newReading } } struct Axis: Hashable { var axisName: String var measurements: [Double] = [] var peaks = 0 var updatesSinceLastPeakCount = 0 /// I love sets, like, a lot. Enough that when I first thought \"but what's an *elegant* way to know when it's a /// good time to count the peaks again?\" I thought of a one-liner set intersection, very semantic, very accurate to the /// underlying question of freshness of sensor data, and it made me happy, and I smiled. /// Anyway, a counter does the same thing with a 0s execution time, here's one of those: mutating func shouldCountPeaks() - Bool { // Peaks are only counted once a second updatesSinceLastPeakCount += 1 if updatesSinceLastPeakCount == MotionManager.updatesPerSecond { updatesSinceLastPeakCount = 0 return true } return false } } @Published var sensors: [ThreeAxisReadings] = [] // MARK: Private private enum SensorNames: String, CaseIterable { case attitude = \"Attitude\" case rotationRate = \"Rotation Rate\" case gravity = \"Gravity\" case userAcceleration = \"User Acceleration\" case acceleration = \"Acceleration\" case gyroscope = \"Gyroscope\" case magnetometer = \"Magnetometer\" } private static let updatesPerSecond: Int = 30 private let motionQueue = OperationQueue() // Don't read sensors on main private let secondsToShow = 5 // Time window to observe private let sensorUpdateInterval = 1.0 / Double(updatesPerSecond) private let manager: CMMotionManager private func startDeviceUpdates(manager _: CMMotionManager) { self.manager .startDeviceMotionUpdates(to: motionQueue) { motion, error in self.collectReadings(motion, error) } self.manager .startAccelerometerUpdates(to: motionQueue) { motion, error in self.collectReadings(motion, error) } self.manager.startGyroUpdates(to: motionQueue) { motion, error in self.collectReadings(motion, error) } self.manager .startMagnetometerUpdates(to: motionQueue) { motion, error in self.collectReadings(motion, error) } } private func collectReadings(_ motion: CMLogItem?, _ error: Error?) { DispatchQueue.main.async { // Add new readings on main switch motion { case let motion as CMDeviceMotion: self.appendReadings( [motion.attitude.pitch, motion.attitude.roll, motion.attitude.yaw], to: \u0026self.sensors[SensorNames.attitude.index()]) self.appendReadings( [motion.rotationRate.x, motion.rotationRate.y, motion.rotationRate.z], to: \u0026self.sensors[SensorNames.rotationRate.index()]) self.appendReadings( [motion.gravity.x, motion.gravity.y, motion.gravity.z], to: \u0026self.sensors[SensorNames.gravity.index()]) self.appendReadings( [motion.userAcceleration.x, motion.userAcceleration.y, motion.userAcceleration.z], to: \u0026self.sensors[SensorNames.userAcceleration.index()]) case let motion as CMAccelerometerData: self.appendReadings( [motion.acceleration.x, motion.acceleration.y, motion.acceleration.z], to: \u0026self.sensors[SensorNames.acceleration.index()]) case let motion as CMGyroData: self.appendReadings( [motion.rotationRate.x, motion.rotationRate.y, motion.rotationRate.z], to: \u0026self.sensors[SensorNames.gyroscope.index()]) case let motion as CMMagnetometerData: self.appendReadings( [motion.magneticField.x, motion.magneticField.y, motion.magneticField.z], to: \u0026self.sensors[SensorNames.magnetometer.index()]) default: print(error != nil ? \"Error: \\(String(describing: error))\" : \"Unknown device\") } } } private func appendReadings( _ newReadings: [Double], to threeAxisReadings: inout ThreeAxisReadings) { for index in 0 ..= Int(1.0 / self .sensorUpdateInterval * Double(self.secondsToShow)) { axis.measurements .removeFirst() // trim old data to keep our moving window representing secondsToShow } threeAxisReadings.axes[index] = axis } } private func countPeaks( in readings: [Double], quantizeFactor: Double) - Int { // Count local maxima let quantizedreadings = readings.map { Int($0 * quantizeFactor) } // Quantize into small Ints (instead of extremely small Doubles) to remove detail from little component waves var ascendingWave = true var numberOfPeaks = 0 var lastReading = 0 for reading in quantizedreadings { if ascendingWave == true, lastReading  reading { // If we were going up but it stopped being true, numberOfPeaks += 1 // we just passed a peak, ascendingWave = false // and we're going down. } else if lastReading Self.AllCases .Index { // Force-unwrap of index of enum case in CaseIterable always succeeds. return Self.allCases.firstIndex(of: self)! } } typealias Sensor = MotionManager.ThreeAxisReadings  Here is a video of my using the completed prototype. It works; you can see that I turn off the sensors which aren\u0026rsquo;t reacting to my pedaling at all, then check the three sensors which do react. I turn off the first two because I don\u0026rsquo;t think the waveform of the bike oscillation is very clear. But in the last one, I can see it quite clearly on axis z. So, I turn on the low-pass filter, turn it up almost all the way, and set the quantizing to a very large number. It accurately counts how often I pedal per window.\nThere should have been a video here but your browser does not seem to support it.  Extro #  âœ‚ - - - Non-code thoughts, please stop reading here for the code-only experience - - - âœ‚\nI haven\u0026rsquo;t been a fan of what I would describe as the Corona Aesthetic in independent filmmaking (2-4 people deal with something ambiguous and hastily-written in a room, or a forest, or a disused building, or online). Not because I\u0026rsquo;m annoyed that no one made pre-Corona-style films for my entertainment during a worldwide crisis; because I believe it was a profoundly creatively-impaired period and it\u0026rsquo;s a collective good to own it.\nEveryone was reckoning with existential fears, and their limbic systems were loudly lit up in unfamiliar, uninteresting ways, and when I watch these films, I feel like I\u0026rsquo;m watching a guy lose the battle to acknowledge to himself that this just isn\u0026rsquo;t going to be the year and he should tend to his own garden, instead of making a public demonstration of his ability to power through his stuck brain problems. As the audience, I feel dropped into the role of validator. Honestly, I feel like feature narrative film is simply too large, rigid, and wasteful a medium for the circumstances.\nBut, oh, the non-narrative short film. Amyl and the Sniffers are a phenomenal punk band out of Melbourne, and John Angus Stewart is a filmmaker from the same town who has made a bunch of their gorgeous videos. For me, this trio of shorts is the best audiovisual summation of the pandemic: what it\u0026rsquo;s like being a little too intense and in an isolated feedback loop with your own energies, longing for connection:\n Guided by Angels Hertz Security  "},{"id":2,"href":"/about/","title":"About | Contact","section":"Introduction","content":"Contact #  You can send me a note here.\nAbout me #  I\u0026rsquo;m Halle (she/her). I\u0026rsquo;ve lived in Berlin for decades but I\u0026rsquo;m originally from New York City. I grew up playing in bands, including one of the first riot grrl bands in NYC, writing music and studying music composition, doing audio engineering, and experimenting in software audio synthesis back when it was done in NeXTSTEP (although I wrote my first audio code ever when I was 10, on a VIC-20). I etched the PCB for my first guitar distortion pedal. It didn\u0026rsquo;t work! Later on, I got a lot better at hardware.\nI spent early adulthood inline-skating around NYC helping companies deal with the first year of the web, and art directed and coded several early-web firsts: the first commercial fashion website, and the first commercial record label website, after which I moved out west and was a double-digit employee of the US\u0026rsquo; first cable broadband company, as a senior technical designer for their daily-published media site; to the best of my knowledge the first site which served inline video as its standard content.\nI had a full and fulfilling career as a graphic designer who could code, and then starting in 2004 I began getting back to my NeXTSTEP programmatic synthesis roots and writing software for Apple platforms, starting with some Final Cut Pro plugins for video post-processing. In 2008 I worked through the CS courses that Stanford University made free and public, started writing apps for the iPhone from year one, and since then have worked on many interesting, beautiful, and challenging things. I really liked Objective-C, I really like Swift, and I guess I\u0026rsquo;m comfortable with change!\nAbout The Offcuts #  Think about the shop: the wood shop, the metal shop, the sewing shop, the print shop. Now think about that box of offcuts: the beautiful, strange, oddly-shaped extra parts that were left over when the other thing was produced. They\u0026rsquo;re free; you can make anything you want with them, if you have an idea.\n"},{"id":3,"href":"/privacypolicy/","title":"Privacy Policy","section":"Introduction","content":"Privacy #  The policy is that you have privacy. This site has no cookies or accounts so there is no way to persist any information about you here. I don\u0026rsquo;t have access to the visitor logs. Your personally identifiable information is not collected by this blog or by me. The webhost is Github Pages, which means it is possible that you have a different privacy relationship with Github, so you can also check out their privacy policy.\nConsequent to the fact that this is a personal blog with no way of storing information about you, there is no DPO because there would be nothing for them to do.\nThe responsible party under the TKG is Halle Winkler, and you can contact me here.\n"},{"id":4,"href":"/workwithhalle/","title":"Work with Halle","section":"Introduction","content":"Work with Halle #  I sometimes have availability for contracts. My preference is a contract of 3-6 months, but I\u0026rsquo;m open-minded, so please feel free to get in touch.\n"}]