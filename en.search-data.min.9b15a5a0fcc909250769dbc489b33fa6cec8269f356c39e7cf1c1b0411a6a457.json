[{"id":0,"href":"/posts/core-media-io-camera-extensions-part-three/","title":"Getting To Grips With The Core Media IO Camera Extension Part 3 of 3: building a creative camera with realtime vImage Pixel Buffer effects using the Continuity Camera Webcam","section":"Posts","content":"Getting To Grips With The Core Media IO Camera Extension, a 3 part series. #  Part 3 of 3: Bringing it all together by building a creative camera with realtime effects processing using vImage Pixel Buffers, which can use the Continuity Camera Webcam. #  August 31st, 2022 To the code →\n View project on Github #  Welcome to the third in a series of three posts about the Core Media IO Camera Extension.\nThe first post, \u0026ldquo;The Basics\u0026rdquo;, is where you can learn what the series is about, its prerequisites, and to understand the project goals, while setting up the project for success.\nThe second was about extending that project by creating a useful software CMIO Camera Extension with communication between a configuration app and an extension, and learning how to do painless extension debugging,\nThis one is about bringing it all together by building a creative camera with realtime effects processing using vImage Pixel Buffers, which can use the Continuity Camera Webcam.\nThe previous two posts supported macOS versions starting with 12.3, but this one is about new APIs in beta, so you will need to have a Ventura beta installed with a matching Xcode beta. I wrote them mostly with Ventura beta 5, so it is probably a good idea to have beta 5 or later.\nYou\u0026rsquo;ll also need a compatible beta OS on your phone in order to try the Continuity Camera features, but Continuity Camera isn\u0026rsquo;t a requirement for working through the last part of this project – there will be fallback code to non-Continuity cameras, just like your extension should probably have, so no worries if you have an older phone, or a newer phone that you don\u0026rsquo;t want to (or aren\u0026rsquo;t allowed to) make a beta victim.\nLet\u0026rsquo;s jump in. In part 2, we created a useful software camera and learned how to debug camera extensions without pain. You can view or clone my version of it, keeping in mind that you will need to change all references to my organization ID (com.politepix) and my team ID to match your own, or it definitely won\u0026rsquo;t work. Ideally you have your own that you made in the previous post and it is still working, so we can continue with it here.\nWe have already dealt with all of the gnarly process topics like how to set up these projects correctly, how to install and uninstall extensions successfully, how very basic interprocess communication works for extensions, how to debug live extensions, and how to strenuously avoid debugging live extensions. With all of that packed away successfully, we can zero in on the fun topics today – live video and realtime effects.\nLoose ends #  I first should tie up something I left open in my previous post. Right at the end, regarding a modification I made to the end-to-end testing app so that the technical difficulties image would display unmirrored, I said this:\n\u0026ldquo;I suspect my vertical-axis flipping in the end-to-end app isn’t correct for pixel buffers that originate from AVCaptureSession video, so I foresee a future improvement in which, instead of flipping the image, it complains when a pixel buffer was created with the wrong format or properties that would result in being in the wrong coordinate space once it gets to the camera system.\u0026rdquo;\nPast me was correct to be suspicious of this fix, but not for the right reasons. The reason the placard was mirrored was not because I wrote a bug, but because it should be mirrored. A system camera defaults to showing the user of the camera everything (usually themselves) in mirror image, because that is how we\u0026rsquo;re used to seeing ourselves. It shows everything (usually ourselves) to the person on the other end of the video call unmirrored, because that is how others are used to seeing us. Which means that the mirrored image wasn\u0026rsquo;t truly mirrored – when I made a test call and looked at the other end of the call, the image was shown correctly without my fix code.\nThis means that the code added to the end-to-end testing app is correct and should remain in place, because it shows us what the enduser should see, but the fix added to the technical difficulties image in the extension code is not correct and should be removed, so that when we see the technical difficulties image in the end-to-end testing app, we see it mirrored, correctly.\nPicking up where we left off #  Never mind the Technical Difficulties, here\u0026rsquo;s the Art Film. Today we\u0026rsquo;re going to do a bunch of different things:\n Shift from a software camera which shows imagery from inside the extension, to a creative camera which alters a live camera feed, Use that extremely fine new Continuity Camera Webcam as our video source, Do some direct interaction from the configuration app to the camera extension using custom properties, and with all of that working, Create realtime video effects for our creative camera using vImage Pixel Buffers, the new hotness for pixels + math from the Accelerate framework.  We have a lot to get through and the changes will be extensive. Where possible, I am going to try to work with entire classes that we can take brief tours through as we add them, but there will be a few line-by-line changes as we implement them. I am going to leave our work just in the four major files we\u0026rsquo;ve been using so far so that this doesn\u0026rsquo;t become a tutorial about adding files to the correct target and which file to switch to, but please feel absolutely free to put each class or struct in its own file afterwards when you want to experiment further; I definitely would separate this into more files in my own ongoing project.\nThroughout this process, you can always refer to the completed sample app for this blog post at Github.\nSome cleanup #  The first thing we need to do is get ready to use a live camera and to use all the beta code in our three targets, so let\u0026rsquo;s review and make sure that all three targets (extension, app, and end-to-end testing app) have a Camera usage sandbox capability, that all three targets have a Privacy – Camera Usage Description key in their Info settings with a valid description such as \u0026quot;Camera Extension\u0026quot;, and all three targets are set to deploy to macOS 13.0 (at least).\nNext, please rename the file NotificationName.swift to Shared.swift because we are going to add some more shared Extension/App code there.\nLastly, please entirely remove the jpeg files Dirty.jpg and Clean.jpg.\nLive camera streaming #  We will start by adding live camera streaming in our ExtensionProvider and making sure it works using the end-to-end testing app. We will do this using AVCaptureSession. Add the following imports to Shared.swift:\nimport AppKit import AVFoundation import CoreMediaIO And now add this class to manage device frame capture to Shared.swift:\n// MARK: - CaptureSessionManager class CaptureSessionManager: NSObject { // MARK: Lifecycle init(capturingOffcutsCam: Bool) { super.init() captureOffcutsCam = capturingOffcutsCam configured = configureCaptureSession() } // MARK: Internal enum Camera: String { case continuityCamera case offcutsCam = \u0026#34;OffcutsCam\u0026#34; } var configured: Bool = false var captureOffcutsCam = false var captureSession: AVCaptureSession = .init() var videoOutput: AVCaptureVideoDataOutput? let dataOutputQueue = DispatchQueue(label: \u0026#34;video_queue\u0026#34;, qos: .userInteractive, attributes: [], autoreleaseFrequency: .workItem) func configureCaptureSession() -\u0026gt; Bool { var result = false switch AVCaptureDevice.authorizationStatus(for: .video) { case .authorized: break case .notDetermined: AVCaptureDevice.requestAccess( for: .video, completionHandler: { granted in if !granted { logger.error(\u0026#34;1. App requires camera access, returning\u0026#34;) return } else { result = self.configureCaptureSession() } } ) return result default: logger.error(\u0026#34;2. App requires camera access, returning\u0026#34;) return false } captureSession.beginConfiguration() captureSession.sessionPreset = sessionPreset guard let camera = getCameraIfAvailable(camera: captureOffcutsCam ? .offcutsCam : .continuityCamera) else { logger.error(\u0026#34;Can\u0026#39;t create default camera, this could be because the extension isn\u0026#39;t installed, returning\u0026#34;) return false } do { let fallbackPreset = AVCaptureSession.Preset.high let input = try AVCaptureDeviceInput(device: camera) let supportStandardPreset = input.device.supportsSessionPreset(sessionPreset) if !supportStandardPreset { let supportFallbackPreset = input.device.supportsSessionPreset(fallbackPreset) if supportFallbackPreset { captureSession.sessionPreset = fallbackPreset } else { logger.error(\u0026#34;No HD formats used by this code supported, returning.\u0026#34;) return false } } captureSession.addInput(input) } catch { logger.error(\u0026#34;Can\u0026#39;t create AVCaptureDeviceInput, returning\u0026#34;) return false } videoOutput = AVCaptureVideoDataOutput() if let videoOutput = videoOutput { if captureSession.canAddOutput(videoOutput) { captureSession.addOutput(videoOutput) captureSession.commitConfiguration() return true } else { logger.error(\u0026#34;Can\u0026#39;t add video output, returning\u0026#34;) return false } } return false } // MARK: Private private let sessionPreset = AVCaptureSession.Preset.hd1280x720 private func getCameraIfAvailable(camera: Camera) -\u0026gt; AVCaptureDevice? { let discoverySession = AVCaptureDevice.DiscoverySession(deviceTypes: [.externalUnknown, .builtInMicrophone, .builtInMicrophone, .builtInWideAngleCamera, .deskViewCamera], mediaType: .video, position: .unspecified) for device in discoverySession.devices { switch camera { case .continuityCamera: if device.isContinuityCamera, device.deviceType != .deskViewCamera { return device } case .offcutsCam: if device.localizedName == camera.rawValue { return device } } } return AVCaptureDevice.userPreferredCamera } } This is in Shared.swift because we will be using slightly different implementations of it in the extension and in the container app. Make sure all three targets can build and make any needed changes (overlooked imports, deployment versions, etc).\nLet\u0026rsquo;s talk briefly about what is going on in this class and what it will do for us in ExtensionProvider once we implement it there.\n In its init(), it finds out if it is supposed to capture the normal camera feed (e.g. the Continuity Camera Webcam, what we are going to use it for right now) or if we are going to use it to capture the feed from the installed extension (not for now). It then does a very standard AVCaptureSession configuration, based on Apple\u0026rsquo;s published best practices. It tries to obtain the camera we specified (e.g. Continuity) by checking for a camera which is continuity but isn\u0026rsquo;t the desk view, and returns it, or falls back to the .userPreferredCamera if that doesn\u0026rsquo;t work out. It sets up and adds the input and output of the session from this camera and if all is good, it commits the configuration and returns true, or logs errors and returns false. When we have set up the captureSession\u0026rsquo;s final sampleBufferDelegate in whichever target is implementing this code, that target will receive the buffers from this configured AVCaptureSession and can do what it likes with them.  Now we can implement it in the extension code.\nIn ExtensionProvider.swift, add the following imports and constants:\nimport Accelerate import AVFoundation let outputWidth = 1280 let outputHeight = 720 Next, go to the ExtensionStreamSource function init(localizedName: String, streamID: UUID, streamFormat: CMIOExtensionStreamFormat, device: CMIOExtensionDevice) and add the following lines right under super.init():\n captureSessionManager = CaptureSessionManager(capturingOffcutsCam: false) guard let captureSessionManager = captureSessionManager else { logger.error(\u0026#34;Not able to get capture session, returning.\u0026#34;) return } guard captureSessionManager.configured == true, let captureSessionManagerOutput = captureSessionManager.videoOutput else { logger.error(\u0026#34;Not able to configure session and change captureSessionManagerOutput delegate, returning\u0026#34;) return } captureSessionManagerOutput.setSampleBufferDelegate(self, queue: captureSessionManager.dataOutputQueue) Add this variable to ExtensionStreamSource :\nprivate var captureSessionManager: CaptureSessionManager?\nAdd the class conformance AVCaptureVideoDataOutputSampleBufferDelegate to ExtensionStreamSource in its class declaration right after CMIOExtensionStreamSource so we can a callback when there are captured buffers in the extension.\nchange the functions func startStream() and func stopStream() to this, so instead of starting the old streaming code they start and stop the captureSession:\n func startStream() throws { guard let captureSessionManager = captureSessionManager, captureSessionManager.captureSession.isRunning == false else { logger.error(\u0026#34;Can\u0026#39;t start capture session running, returning\u0026#34;) return } captureSessionManager.captureSession.startRunning() } func stopStream() throws { guard let captureSessionManager = captureSessionManager, captureSessionManager.configured, captureSessionManager.captureSession.isRunning else { logger.error(\u0026#34;Can\u0026#39;t stop AVCaptureSession where it is expected, returning\u0026#34;) return } if captureSessionManager.captureSession.isRunning { captureSessionManager.captureSession.stopRunning() } } and add this function (this is the delegate callback we set up in the previous lines and conformance to receive captured sampleBuffers from the camera) to ExtensionStreamSource so we have captured buffers streaming in:\n func captureOutput(_: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from _: AVCaptureConnection) { guard let deviceSource = device.source as? ExtensionDeviceSource else { logger.error(\u0026#34;Couldn\u0026#39;t obtain device source\u0026#34;) return } guard let pixelBuffer = sampleBuffer.imageBuffer else { return } CVPixelBufferLockBaseAddress( pixelBuffer, CVPixelBufferLockFlags.readOnly) var err: OSStatus = 0 var sbuf: CMSampleBuffer! var timingInfo = CMSampleTimingInfo() timingInfo.presentationTimeStamp = CMClockGetTime(CMClockGetHostTimeClock()) var formatDescription: CMFormatDescription? CMVideoFormatDescriptionCreateForImageBuffer(allocator: kCFAllocatorDefault, imageBuffer: pixelBuffer, formatDescriptionOut: \u0026amp;formatDescription) err = CMSampleBufferCreateReadyWithImageBuffer(allocator: kCFAllocatorDefault, imageBuffer: pixelBuffer, formatDescription: formatDescription!, sampleTiming: \u0026amp;timingInfo, sampleBufferOut: \u0026amp;sbuf) if err == 0 { if deviceSource._isExtension { // If I\u0026#39;m the extension, send to output stream stream.send(sbuf, discontinuity: [], hostTimeInNanoseconds: UInt64(timingInfo.presentationTimeStamp.seconds * Double(NSEC_PER_SEC))) } else { deviceSource.extensionDeviceSourceDelegate?.bufferReceived(sbuf) // If I\u0026#39;m the end to end testing app, send to delegate method. } } else { logger.error(\u0026#34;Error in stream: \\(err)\u0026#34;) } CVPixelBufferUnlockBaseAddress( pixelBuffer, CVPixelBufferLockFlags.readOnly) } remove the private designations from the variables private var _isExtension: Bool = true and private var _videoDescription: CMFormatDescription! and private var _streamSource: ExtensionStreamSource!\nAs you can see, we have moved the function where we handle streaming buffers from ExtensionDeviceSource to ExtensionStreamSource, as a consequence of the fact that we are starting and stopping the capture session in the stream (where it makes sense to do that) and that is also the class that gets the callback for the captured buffers. This feels natural to me, since these are all characteristics of the video stream.\nDelete the contents of func startStreaming() in ExtensionDeviceSource but leave the function in place for now.\nThis should build. But, if we build and run the end-to-end testing app, we will get the \u0026ldquo;no video\u0026rdquo; image. This is because this more-complex extension requires some changes in the way the end-to-end app works so that it continues to emulate the system extension machinery.\nWe\u0026rsquo;ll make a couple of quick changes here. I am going to show you a more powerful way to communicate between the container app and extension later on, so let\u0026rsquo;s repurpose our simple Darwin CFNotification system so that it is just for the end-to-end testing app. Add the following class to Shared.swift:\n // MARK: - NotificationManager class NotificationManager { class func postNotification(named notificationName: String) { let completeNotificationName = Identifiers.appGroup.rawValue + \u0026#34;.\u0026#34; + notificationName logger .debug( \u0026#34;Posting notification \\(completeNotificationName) from container app\u0026#34; ) CFNotificationCenterPostNotification( CFNotificationCenterGetDarwinNotifyCenter(), CFNotificationName(completeNotificationName as NSString), nil, nil, true ) } class func postNotification(named notificationName: NotificationName) { logger .debug( \u0026#34;Posting notification \\(notificationName.rawValue) from container app\u0026#34; ) CFNotificationCenterPostNotification( CFNotificationCenterGetDarwinNotifyCenter(), CFNotificationName(notificationName.rawValue as NSString), nil, nil, true ) } } and replace the contents of the enum enum NotificationName so it looks like this:\nenum NotificationName: String, CaseIterable { case startStream = \u0026#34;Z39BRGKSRW.com.politepix.OffcutsCam.startStream\u0026#34; case stopStream = \u0026#34;Z39BRGKSRW.com.politepix.OffcutsCam.stopStream\u0026#34; } And add this enum:\nenum Identifiers: String { case appGroup = \u0026#34;Z39BRGKSRW.com.politepix.OffcutsCam\u0026#34; case orgIDAndProduct = \u0026#34;com.politepix.OffcutsCam\u0026#34; } Since we\u0026rsquo;ve added the identifiers, let\u0026rsquo;s fix our two logger lines to reference them:\nlet logger = Logger(subsystem: Identifiers.orgIDAndProduct.rawValue.lowercased(), category: \u0026#34;Application\u0026#34;) and\nlet logger = Logger(subsystem: Identifiers.orgIDAndProduct.rawValue.lowercased(), category: \u0026#34;Extension\u0026#34;) In ExtensionProvider.swift, remove the following notifications functions:\n private func notificationReceived(notificationName: String) { guard let name = NotificationName(rawValue: notificationName) else { return } switch name { case .changeImage: self.deviceSource.imageIsClean.toggle() logger.debug(\u0026#34;The camera extension has received a notification\u0026#34;) logger.debug(\u0026#34;The notification is: \\(name.rawValue)\u0026#34;) self.deviceSource.stopStreaming() self.deviceSource.startStreaming() } } private func startNotificationListeners() { for notificationName in NotificationName.allCases { let observer = UnsafeRawPointer(Unmanaged.passUnretained(self).toOpaque()) CFNotificationCenterAddObserver(CFNotificationCenterGetDarwinNotifyCenter(), observer, { _, observer, name, _, _ in if let observer = observer, let name = name { let extensionProviderSourceSelf = Unmanaged\u0026lt;ExtensionProviderSource\u0026gt;.fromOpaque(observer).takeUnretainedValue() extensionProviderSourceSelf.notificationReceived(notificationName: name.rawValue as String) } }, notificationName.rawValue as CFString, nil, .deliverImmediately) } } private func stopNotificationListeners() { if notificationListenerStarted { CFNotificationCenterRemoveEveryObserver(notificationCenter, Unmanaged.passRetained(self) .toOpaque()) notificationListenerStarted = false } } And then add this extension to the end of the file so we can support operating the ExtensionProvider with notifications when it isn\u0026rsquo;t part of the system machinery, with better modularization as this file becomes more complex:\n extension ExtensionProviderSource { private func notificationReceived(notificationName: String) { if let name = NotificationName(rawValue: notificationName) { switch name { case .startStream: do { try deviceSource._streamSource.startStream() } catch { logger.debug(\u0026#34;Couldn\u0026#39;t start the stream\u0026#34;) } case .stopStream: do { try deviceSource._streamSource.stopStream() } catch { logger.debug(\u0026#34;Couldn\u0026#39;t stop the stream\u0026#34;) } } } } private func startNotificationListeners() { var allNotifications = [String]() for notificationName in NotificationName.allCases { allNotifications.append(notificationName.rawValue) } for notificationName in allNotifications { let observer = UnsafeRawPointer(Unmanaged.passUnretained(self).toOpaque()) CFNotificationCenterAddObserver(CFNotificationCenterGetDarwinNotifyCenter(), observer, { _, observer, name, _, _ in if let observer = observer, let name = name { let extensionProviderSourceSelf = Unmanaged\u0026lt;ExtensionProviderSource\u0026gt;.fromOpaque(observer).takeUnretainedValue() extensionProviderSourceSelf.notificationReceived(notificationName: name.rawValue as String) } }, notificationName as CFString, nil, .deliverImmediately) } } private func stopNotificationListeners() { if notificationListenerStarted { CFNotificationCenterRemoveEveryObserver(notificationCenter, Unmanaged.passRetained(self) .toOpaque()) notificationListenerStarted = false } } } We have added a way to forward the end-to-end app\u0026rsquo;s notifications into the ExtensionProvider, which lets us start the stream more like the system, now that we have improved the internal design of the ExtensionProvider.\nThis should now build and run the end-to-end testing app, but it won\u0026rsquo;t change the behavior in the app until we use these new hooks in the end-to-end testing app\u0026rsquo;s ContentView.swift.\nChange the EndToEndStreamProvider override init() in the end-to-end testing app\u0026rsquo;s ContentView.swift so it invokes the ExtensionProvider' startStreaming() call via notification:\n override init() { providerSource = ExtensionProviderSource(clientQueue: nil) super.init() providerSource .deviceSource = ExtensionDeviceSource(localizedName: \u0026#34;OffcutsCam\u0026#34;) providerSource.deviceSource.extensionDeviceSourceDelegate = self NotificationManager .postNotification( named: NotificationName.startStream ) } Now, when you run the end-to-end testing app, you should see the Continuity Camera feed, or it should fall back to your preferred camera if there is no Continuity Camera. If you see nothing at all but you don\u0026rsquo;t get an error, it is very important to make sure that your Continuity Camera, or your other camera if you aren\u0026rsquo;t testing a Continuity Camera now, is definitely working in FaceTime. In the current Ventura betas, Continuity Camera can enter a weird state where the code finds it, and it reports a feed, but the feed is empty, and this prevents it from falling back to a working other camera or returning a \u0026ldquo;no camera\u0026rdquo; error, but no sample buffers are provided. I have seen this behavior several times, but never in a user session where the Continuity Camera Webcam was working normally in FaceTime. If it isn\u0026rsquo;t working in FaceTime, restart your machine and your phone and try again.\nWe\u0026rsquo;ve gotten a live feed from the Continuity Camera and modified our end-to-end testing app so it can control our redesigned extension code, in order to maintain our painless debugging approach. Now that it works, you could also install the extension and reboot, if you felt like it. FaceTime should be able to use your camera afterwards, and it should behave like the Continuity Camera.\nIf everything is working so far, congrats! Get up and take a walk.\nCommand and control #  Back already? Cool, let\u0026rsquo;s add some more direct control from the app to the extension so we can prepare for our special effects. In the last post, part 2, I explained how to use the very simple interprocess communication method of Darwin CFNotification, but now we will want more precise control, and in any case, we\u0026rsquo;ve retired CFNotification to usage by the end-to-end testing app exclusively. We will create custom properties for the camera extension that the container app can interface with directly.\nFirst, it could be a good idea to watch the part of the WWDC22 CMIO Camera Extension video about this, because it\u0026rsquo;s vital context and it will also clarify why we\u0026rsquo;re about to get some ugly wrappers.\nUgly wrappers #  CMIO Camera Extension custom properties are a C API. If you\u0026rsquo;ve done lower-level audio or video code, you know that sometimes we need wrappers to deal nicely with highly-performant or highly-lightweight C and C++ system APIs; if this is your first time, welcome to Gnarlyville! The main goal is to give yourself some tools so that you debug in a single place through a single interface for these translations, and then, with any luck, never think about it again.\nFirst stop, still in Swift-land, add the following to Shared.swift, which are going to eventually control our special effects and give us a needed helpful extra String function:\n// MARK: - MoodName enum MoodName: String, CaseIterable { case bypass = \u0026#34;Bypass\u0026#34; case newWave = \u0026#34;New Wave\u0026#34; case berlin = \u0026#34;Berlin\u0026#34; case oldFilm = \u0026#34;OldFilm\u0026#34; case sunset = \u0026#34;Sunset\u0026#34; case badEnergy = \u0026#34;BadEnergy\u0026#34; case beyondTheBeyond = \u0026#34;BeyondTheBeyond\u0026#34; case drama = \u0026#34;Drama\u0026#34; } // MARK: - PropertyName enum PropertyName: String, CaseIterable { case mood } extension String { func convertedToCMIOObjectPropertySelectorName() -\u0026gt; CMIOObjectPropertySelector { let noName: CMIOObjectPropertySelector = 0 if count == MemoryLayout\u0026lt;CMIOObjectPropertySelector\u0026gt;.size { return data(using: .utf8, allowLossyConversion: false)?.withUnsafeBytes { propertySelector in propertySelector.load(as: CMIOObjectPropertySelector.self).byteSwapped } ?? noName } else { return noName } } } Open the ContentView.swift of the container app target (not the end-to-end app, which cannot directly access custom properties because it uses ExtensionProvider as part of its included code, rather than interacting with it as a system device whose service exposes properties). Add the following imports:\nimport AVFoundation import CoreMediaIO and then add the following class:\n// MARK: - CustomPropertyManager class CustomPropertyManager: NSObject { // MARK: Lifecycle override init() { super.init() effect = MoodName(rawValue: getPropertyValue(withSelectorName: mood) ?? MoodName.bypass.rawValue) ?? MoodName.bypass } // MARK: Internal let mood = PropertyName.mood.rawValue.convertedToCMIOObjectPropertySelectorName() var effect: MoodName = .bypass lazy var deviceObjectID: CMIOObjectID? = { let device = getExtensionDevice(name: \u0026#34;OffcutsCam\u0026#34;) if let device = device, let deviceObjectId = getCMIODeviceID(fromUUIDString: device.uniqueID) { return deviceObjectId } return nil }() func getExtensionDevice(name: String) -\u0026gt; AVCaptureDevice? { let discoverySession = AVCaptureDevice.DiscoverySession(deviceTypes: [.externalUnknown], mediaType: .video, position: .unspecified) return discoverySession.devices.first { $0.localizedName == name } } func propertyExists(inDeviceAtID deviceID: CMIODeviceID, withSelectorName selectorName: CMIOObjectPropertySelector) -\u0026gt; CMIOObjectPropertyAddress? { var address = CMIOObjectPropertyAddress(mSelector: CMIOObjectPropertySelector(selectorName), mScope: CMIOObjectPropertyScope(kCMIOObjectPropertyScopeGlobal), mElement: CMIOObjectPropertyElement(kCMIOObjectPropertyElementMain)) let exists = CMIOObjectHasProperty(deviceID, \u0026amp;address) return exists ? address : nil } func getCMIODeviceID(fromUUIDString uuidString: String) -\u0026gt; CMIOObjectID? { var propertyDataSize: UInt32 = 0 var dataUsed: UInt32 = 0 var cmioObjectPropertyAddress = CMIOObjectPropertyAddress(mSelector: CMIOObjectPropertySelector(kCMIOHardwarePropertyDevices), mScope: CMIOObjectPropertyScope(kCMIOObjectPropertyScopeGlobal), mElement: CMIOObjectPropertyElement(kCMIOObjectPropertyElementMain)) CMIOObjectGetPropertyDataSize(CMIOObjectPropertySelector(kCMIOObjectSystemObject), \u0026amp;cmioObjectPropertyAddress, 0, nil, \u0026amp;propertyDataSize) let count = Int(propertyDataSize) / MemoryLayout\u0026lt;CMIOObjectID\u0026gt;.size var cmioDevices = [CMIOObjectID](repeating: 0, count: count) CMIOObjectGetPropertyData(CMIOObjectPropertySelector(kCMIOObjectSystemObject), \u0026amp;cmioObjectPropertyAddress, 0, nil, propertyDataSize, \u0026amp;dataUsed, \u0026amp;cmioDevices) for deviceObjectID in cmioDevices { cmioObjectPropertyAddress.mSelector = CMIOObjectPropertySelector(kCMIODevicePropertyDeviceUID) CMIOObjectGetPropertyDataSize(deviceObjectID, \u0026amp;cmioObjectPropertyAddress, 0, nil, \u0026amp;propertyDataSize) var deviceName: NSString = \u0026#34;\u0026#34; CMIOObjectGetPropertyData(deviceObjectID, \u0026amp;cmioObjectPropertyAddress, 0, nil, propertyDataSize, \u0026amp;dataUsed, \u0026amp;deviceName) if String(deviceName) == uuidString { return deviceObjectID } } return nil } func getPropertyValue(withSelectorName selectorName: CMIOObjectPropertySelector) -\u0026gt; String? { var propertyAddress = CMIOObjectPropertyAddress(mSelector: CMIOObjectPropertySelector(selectorName), mScope: CMIOObjectPropertyScope(kCMIOObjectPropertyScopeGlobal), mElement: CMIOObjectPropertyElement(kCMIOObjectPropertyElementMain)) guard let deviceID = deviceObjectID else { logger.error(\u0026#34;Couldn\u0026#39;t get object ID, returning\u0026#34;) return nil } if CMIOObjectHasProperty(deviceID, \u0026amp;propertyAddress) { var propertyDataSize: UInt32 = 0 CMIOObjectGetPropertyDataSize(deviceID, \u0026amp;propertyAddress, 0, nil, \u0026amp;propertyDataSize) var name: NSString = \u0026#34;\u0026#34; var dataUsed: UInt32 = 0 CMIOObjectGetPropertyData(deviceID, \u0026amp;propertyAddress, 0, nil, propertyDataSize, \u0026amp;dataUsed, \u0026amp;name) return name as String } return nil } func setPropertyValue(withSelectorName selectorName: CMIOObjectPropertySelector, to value: String) -\u0026gt; Bool { guard let deviceID = deviceObjectID, var propertyAddress = propertyExists(inDeviceAtID: deviceID, withSelectorName: selectorName) else { logger.debug(\u0026#34;Property doesn\u0026#39;t exist\u0026#34;) return false } var settable: DarwinBoolean = false CMIOObjectIsPropertySettable(deviceID, \u0026amp;propertyAddress, \u0026amp;settable) if settable == false { logger.debug(\u0026#34;Property can\u0026#39;t be set\u0026#34;) return false } var dataSize: UInt32 = 0 CMIOObjectGetPropertyDataSize(deviceID, \u0026amp;propertyAddress, 0, nil, \u0026amp;dataSize) var changedValue: NSString = value as NSString let result = CMIOObjectSetPropertyData(deviceID, \u0026amp;propertyAddress, 0, nil, dataSize, \u0026amp;changedValue) if result != 0 { logger.debug(\u0026#34;Not successful setting property data\u0026#34;) return false } return true } } Let\u0026rsquo;s run through this very quickly. After we create a CustomerPropertyManager, we can make get and set calls to the desired extension property selector, first obtaining the system extension object ID via a Swift string that is the name of the camera (\u0026ldquo;OffcutsCam\u0026rdquo;) and then using that ID to query whether our selector exists on the object (i.e. find out if it has a published property matching our property \u0026ldquo;mood\u0026rdquo;), and if so, set or get it. Our selector is created from a four-letter string (\u0026ldquo;mood\u0026rdquo;) and turned into a CMIOObjectPropertySelector, which is a FourChar. We use our extension on String from Shared.swift for that part. That\u0026rsquo;s basically it.\nWhen I hit issues with my property implementation, I used this project for a sink Camera extension as a reference, so thank you Laurent Denoue for providing a clear working example of property access so I could improve mine, especially given how time-consuming it can be to debug this area of an extension if it\u0026rsquo;s unclear whether the issue is in the extension or the container app.\nLet\u0026rsquo;s clean up the view a little bit and make some changes. Replace all of ContentView with the following:\nstruct ContentView { // MARK: Lifecycle init(systemExtensionRequestManager: SystemExtensionRequestManager, propertyManager: CustomPropertyManager) { self.propertyManager = propertyManager self.systemExtensionRequestManager = systemExtensionRequestManager effect = moods.firstIndex(of: propertyManager.effect) ?? 0 } // MARK: Internal var propertyManager: CustomPropertyManager @ObservedObject var systemExtensionRequestManager: SystemExtensionRequestManager // MARK: Private private var moods = MoodName.allCases @State private var effect: Int } // MARK: View extension ContentView: View { var body: some View { VStack { Button(\u0026#34;Install\u0026#34;, action: { systemExtensionRequestManager.install() }) Button(\u0026#34;Uninstall\u0026#34;, action: { systemExtensionRequestManager.uninstall() }) Picker(selection: $effect, label: Text(\u0026#34;Effect\u0026#34;)) { ForEach(Array(moods.enumerated()), id: \\.offset) { index, element in Text(element.rawValue).tag(index) } } .pickerStyle(.segmented) .onChange(of: effect) { tag in let result = propertyManager.setPropertyValue(withSelectorName: propertyManager.mood, to: moods[tag].rawValue) logger.debug(\u0026#34;Setting new property value (\\\u0026#34;\\(propertyManager.getPropertyValue(withSelectorName: propertyManager.mood) ?? \u0026#34;Unknown new string\u0026#34;)\\\u0026#34;) was \\(result ? \u0026#34;successful\u0026#34; : \u0026#34;unsuccessful\u0026#34;)\u0026#34;) } .disabled(propertyManager.deviceObjectID == nil) Text(systemExtensionRequestManager.logText) } .frame(alignment: .top) Spacer() } } And fix everything that initializes this view so it looks like this:\nContentView(systemExtensionRequestManager: SystemExtensionRequestManager(logText: \u0026#34;\u0026#34;), propertyManager: CustomPropertyManager()) Now, in ExtensionProvider.swift, in ExtensionDeviceSource, add this variable:\nvar mood = MoodName.bypass\nand replace all the content of these property management functions of ExtensionDeviceSource:\n var availableProperties: Set\u0026lt;CMIOExtensionProperty\u0026gt;, func deviceProperties(forProperties properties: Set\u0026lt;CMIOExtensionProperty\u0026gt;) throws -\u0026gt; CMIOExtensionDeviceProperties, and func setDeviceProperties(_ deviceProperties: CMIOExtensionDeviceProperties) throws  with the following block that includes returned data and behavior for our custom property:\n var availableProperties: Set\u0026lt;CMIOExtensionProperty\u0026gt; { [.deviceTransportType, .deviceModel, customEffectExtensionProperty] } func deviceProperties(forProperties properties: Set\u0026lt;CMIOExtensionProperty\u0026gt;) throws -\u0026gt; CMIOExtensionDeviceProperties { let deviceProperties = CMIOExtensionDeviceProperties(dictionary: [:]) if properties.contains(.deviceTransportType) { deviceProperties.transportType = kIOAudioDeviceTransportTypeVirtual } if properties.contains(.deviceModel) { deviceProperties.model = \u0026#34;OffcutsCam Model\u0026#34; } // If I get there and there is a key for my effect, that means that we\u0026#39;ve run before. // We are backing the custom property with the extension\u0026#39;s UserDefaults. let userDefaultsPropertyKey = PropertyName.mood.rawValue if userDefaults?.object(forKey: userDefaultsPropertyKey) != nil, let propertyMood = userDefaults?.string(forKey: userDefaultsPropertyKey) { // Not first run deviceProperties.setPropertyState(CMIOExtensionPropertyState(value: propertyMood as NSString), forProperty: customEffectExtensionProperty) if let moodName = MoodName(rawValue: propertyMood) { mood = moodName } } else { // We have never run before, so set property and the backing UserDefaults to default setting deviceProperties.setPropertyState(CMIOExtensionPropertyState(value: MoodName.bypass.rawValue as NSString), forProperty: customEffectExtensionProperty) userDefaults?.set(MoodName.bypass.rawValue, forKey: userDefaultsPropertyKey) logger.debug(\u0026#34;Did initial set of effects value to \\(MoodName.bypass.rawValue)\u0026#34;) mood = MoodName.bypass } return deviceProperties } func setDeviceProperties(_ deviceProperties: CMIOExtensionDeviceProperties) throws { let userDefaultsPropertyKey = PropertyName.mood.rawValue if let customEffectValueFromPropertiesDictionary = dictionaryValueForEffectProperty(in: deviceProperties) { logger.debug(\u0026#34;New setting in device properties for custom effect property: \\(customEffectValueFromPropertiesDictionary)\u0026#34;) userDefaults?.set(customEffectValueFromPropertiesDictionary, forKey: userDefaultsPropertyKey) if let moodName = MoodName(rawValue: customEffectValueFromPropertiesDictionary) { mood = moodName } } } private let customEffectExtensionProperty: CMIOExtensionProperty = .init(rawValue: \u0026#34;4cc_\u0026#34; + PropertyName.mood.rawValue + \u0026#34;_glob_0000\u0026#34;) // Custom \u0026#39;effect\u0026#39; property private let userDefaults = UserDefaults(suiteName: Identifiers.appGroup.rawValue) private func dictionaryValueForEffectProperty(in deviceProperties: CMIOExtensionDeviceProperties) -\u0026gt; String? { guard let customEffectValueFromPropertiesDictionary = deviceProperties.propertiesDictionary[customEffectExtensionProperty]?.value as? String else { logger.debug(\u0026#34;Was not able to get the value of the custom effect property from the properties dictionary of the device, returning.\u0026#34;) return nil } return customEffectValueFromPropertiesDictionary } We have created a custom property called \u0026ldquo;mood\u0026rdquo;. The entire property name is the string \u0026quot;4cc_mood_glob_0000\u0026quot; which is a standard prefix, the name of our property selector, the scope (global), and a standard suffix. In practice it means that there is a published selector for this extension which a client can connect to using the FourChar \u0026ldquo;mood\u0026rdquo;, which should sound familiar after the code we added to the container app. This \u0026ldquo;mood\u0026rdquo; is eventually going to provide the extension with some information about which effect we want.\nBringing it all together, when we select an option in the container app picker, it obtains a reference to the device ID and this custom selector, and changes the value of the property, and the extension will react to this and also back the property change with an entry in its UserDefaults for persistence.\nIn the container app UI, to debug this, every time the custom property is set in the UI, it also does a get so it can report the changed status to you.\nI am very sorry to tell you that this is one part of the development process which requires reboots and reinstalls to troubleshoot, if properties aren\u0026rsquo;t working quite right within your extension code. As mentioned, exposing the extension properties is a function of the system extension machinery, so there isn\u0026rsquo;t any real debug feedback on this outside of that context. But I have already tested out this code, so if you follow these instructions, it shouldn\u0026rsquo;t need debugging. At this point, you should be able to build, run, install the new extension from the container app, quit the app, build and run the app again, and see feedback in the debugger when you make selections. Between app sessions, your choices will persist.\nOne very important other thing, so we don\u0026rsquo;t bring all of our fast debugging to a halt with this improvement: let\u0026rsquo;s add some code to the end-to-end testing app ContentView that will at least allow us to test the consequences of property changes from the end-to-end testing app, even if it can\u0026rsquo;t access the actual properties. Add these variables to the end-to-end testing app\u0026rsquo;s ContentView:\n@State var effect: Int var moods = MoodName.allCases and add this familiar-looking picker in the view:\n Picker(selection: $effect, label: Text(\u0026#34;Effect\u0026#34;)) { ForEach(Array(moods.enumerated()), id: \\.offset) { index, element in Text(element.rawValue).tag(index) } } .pickerStyle(.segmented) .onChange(of: effect) { tag in logger.debug(\u0026#34;Chosen effect: \\(moods[tag].rawValue)\u0026#34;) NotificationManager.postNotification(named: moods[tag].rawValue) } And change how the ContentView is initialized, anywhere you need to:\nContentView(endToEndStreamProvider: EndToEndStreamProvider(), effect: 0) Change our ExtensionProvider extension\u0026rsquo;s function private func startNotificationListeners() to this:\n private func startNotificationListeners() { var allNotifications = [String]() for notificationName in NotificationName.allCases { allNotifications.append(notificationName.rawValue) } for notificationName in MoodName.allCases { allNotifications.append(Identifiers.appGroup.rawValue + \u0026#34;.\u0026#34; + notificationName.rawValue) } for notificationName in allNotifications { let observer = UnsafeRawPointer(Unmanaged.passUnretained(self).toOpaque()) CFNotificationCenterAddObserver(CFNotificationCenterGetDarwinNotifyCenter(), observer, { _, observer, name, _, _ in if let observer = observer, let name = name { let extensionProviderSourceSelf = Unmanaged\u0026lt;ExtensionProviderSource\u0026gt;.fromOpaque(observer).takeUnretainedValue() extensionProviderSourceSelf.notificationReceived(notificationName: name.rawValue as String) } }, notificationName as CFString, nil, .deliverImmediately) } } and change its private func notificationReceived(notificationName: String) to this:\n private func notificationReceived(notificationName: String) { if let name = NotificationName(rawValue: notificationName) { switch name { case .startStream: do { try deviceSource._streamSource.startStream() } catch { logger.debug(\u0026#34;Couldn\u0026#39;t start the stream\u0026#34;) } case .stopStream: do { try deviceSource._streamSource.stopStream() } catch { logger.debug(\u0026#34;Couldn\u0026#39;t stop the stream\u0026#34;) } } } else { if let mood = MoodName(rawValue: notificationName.replacingOccurrences(of: Identifiers.appGroup.rawValue + \u0026#34;.\u0026#34;, with: \u0026#34;\u0026#34;)) { deviceSource.mood = mood } } } Which will forward the same mood info into ExtensionProvider when there are no properties to interact with because we\u0026rsquo;re running it directly in the end-to-end testing app.\nMaybe make a nice cup of tea? See you in a bit.\nEffects! #  Oh, hello! We\u0026rsquo;re finally ready for the effects. Please download the following 7 images and add them to your Extension target and your OffcutsCamEndToEnd target. Please make extra sure that they are added to both.\n 1.jpg 2.jpg 3.jpg 4.jpg 5.jpg 6.jpg 7.jpg  Confirm that they have been successfully added by building the container app and listing the contents of its directory OffcutsCam.app/Contents/Library/SystemExtensions/com.politepix.OffcutsCam.Extension.systemextension/Contents/Resources/, (replacing my bundle ID with yours) which is where they should be.\nAdd the following import and variable to the top of ExtensionProvider:\nimport AppKit let pixelBufferSize = vImage.Size(width: outputWidth, height: outputHeight) and set let kFrameRate: Int = 1 to let kFrameRate: Int = 24.\nAdd this class to ExtensionProvider:\n // MARK: - Effects class Effects: NSObject { // MARK: Lifecycle // Effect processing with vImage Pixel Buffers override init() { super.init() if let image = NSImage(named: \u0026#34;1.jpg\u0026#34;) { // Get histograms for all the chosen images in init sourceImageHistogramNewWave = getHistogram(for: image) } if let image = NSImage(named: \u0026#34;2.jpg\u0026#34;) { sourceImageHistogramBerlin = getHistogram(for: image) } if let image = NSImage(named: \u0026#34;3.jpg\u0026#34;) { sourceImageHistogramOldFilm = getHistogram(for: image) } if let image = NSImage(named: \u0026#34;4.jpg\u0026#34;) { sourceImageHistogramSunset = getHistogram(for: image) } if let image = NSImage(named: \u0026#34;5.jpg\u0026#34;) { sourceImageHistogramBadEnergy = getHistogram(for: image) } if let image = NSImage(named: \u0026#34;6.jpg\u0026#34;) { sourceImageHistogramBeyondTheBeyond = getHistogram(for: image) } if let image = NSImage(named: \u0026#34;7.jpg\u0026#34;) { sourceImageHistogramDrama = getHistogram(for: image) } let randomNumberGenerator = BNNSCreateRandomGenerator( BNNSRandomGeneratorMethodAES_CTR, nil)! for _ in 0 ..\u0026lt; maximumNoiseArrays { // Get random noise for all the noise buffers in init let noiseBuffer = vImage.PixelBuffer( size: pixelBufferSize, pixelFormat: vImage.InterleavedFx3.self) let shape = BNNS.Shape.tensor3DFirstMajor( noiseBuffer.width, noiseBuffer.height, noiseBuffer.channelCount) noiseBuffer.withUnsafeMutableBufferPointer { noisePtr in if var descriptor = BNNSNDArrayDescriptor( data: noisePtr, shape: shape) { let mean: Float = 0.0125 let stdDev: Float = 0.025 BNNSRandomFillNormalFloat(randomNumberGenerator, \u0026amp;descriptor, mean, stdDev) } } noiseBufferArray.append(noiseBuffer) } } // MARK: Internal let cvImageFormat = vImageCVImageFormat.make( format: .format422YpCbCr8, matrix: kvImage_ARGBToYpCbCrMatrix_ITU_R_601_4.pointee, chromaSiting: .center, colorSpace: CGColorSpaceCreateDeviceRGB(), alphaIsOpaqueHint: true)! var cgImageFormat = vImage_CGImageFormat( bitsPerComponent: 32, bitsPerPixel: 32 * 3, colorSpace: CGColorSpaceCreateDeviceRGB(), bitmapInfo: CGBitmapInfo( rawValue: CGBitmapInfo.byteOrder32Little.rawValue | CGBitmapInfo.floatComponents.rawValue | CGImageAlphaInfo.none.rawValue), renderingIntent: .defaultIntent)! let destinationBuffer = vImage.PixelBuffer( size: pixelBufferSize, pixelFormat: vImage.InterleavedFx3.self) func populateDestinationBuffer(pixelBuffer: CVPixelBuffer) { // Convert into destinationBuffer content let sourceBuffer = vImage.PixelBuffer( referencing: pixelBuffer, converter: converter, destinationPixelFormat: vImage.DynamicPixelFormat.self) do { try converter.convert( from: sourceBuffer, to: destinationBuffer) } catch { fatalError(\u0026#34;Any-to-any conversion failure.\u0026#34;) } } func artFilm(forMood mood: MoodName) { // Apply mood to frame tastefulNoise(destinationBuffer: destinationBuffer) specifySavedHistogram(forMood: mood) mildTemporalBlur() } // MARK: Private private lazy var converter: vImageConverter = { guard let converter = try? vImageConverter.make( sourceFormat: cvImageFormat, destinationFormat: cgImageFormat) else { fatalError(\u0026#34;Unable to create converter\u0026#34;) } return converter }() private lazy var temporalBuffer = vImage.PixelBuffer( // temporal blur storage size: pixelBufferSize, pixelFormat: vImage.InterleavedFx3.self) private lazy var histogramBuffer = vImage.PixelBuffer( // Temp histogram storage size: pixelBufferSize, pixelFormat: vImage.PlanarFx3.self) private var noiseBufferArray: [vImage.PixelBuffer\u0026lt;vImage.InterleavedFx3\u0026gt;] = .init() private var sourceImageHistogramNewWave: vImage.PixelBuffer.HistogramFFF? // All set up before applying effect private var sourceImageHistogramBerlin: vImage.PixelBuffer.HistogramFFF? private var sourceImageHistogramOldFilm: vImage.PixelBuffer.HistogramFFF? private var sourceImageHistogramSunset: vImage.PixelBuffer.HistogramFFF? private var sourceImageHistogramBadEnergy: vImage.PixelBuffer.HistogramFFF? private var sourceImageHistogramBeyondTheBeyond: vImage.PixelBuffer.HistogramFFF? private var sourceImageHistogramDrama: vImage.PixelBuffer.HistogramFFF? private let maximumNoiseArrays = kFrameRate / 2 // How many noise arrays we\u0026#39;ll use for faking continuous random noise private var noiseArrayCount = 0 private var noiseArrayCountAscending = true private let histogramBinCount = 32 private func mildTemporalBlur() { let interpolationConstant: Float = 0.4 destinationBuffer.linearInterpolate( bufferB: temporalBuffer, interpolationConstant: interpolationConstant, destination: temporalBuffer) temporalBuffer.copy(to: destinationBuffer) } private func tastefulNoise(destinationBuffer: vImage.PixelBuffer\u0026lt;vImage.InterleavedFx3\u0026gt;) { guard noiseBufferArray.count == maximumNoiseArrays else { return } destinationBuffer.withUnsafeMutableBufferPointer { mutableDestintationPtr in vDSP.add(destinationBuffer, noiseBufferArray[noiseArrayCount], result: \u0026amp;mutableDestintationPtr) } if noiseArrayCount == maximumNoiseArrays - 1 { noiseArrayCountAscending = false } else if noiseArrayCount == 0 { if noiseArrayCountAscending == false { // the maximumNoiseArrays * 2 pass, we shuffle so the eyes don\u0026#39;t start to notice patterns in the \u0026#34;noise dance\u0026#34; noiseBufferArray = noiseBufferArray.shuffled() } noiseArrayCountAscending = true } if noiseArrayCountAscending { noiseArrayCount += 1 } else { noiseArrayCount -= 1 } } private func specifySavedHistogram(forMood mood: MoodName) { var sourceHistogramToSpecify = sourceImageHistogramNewWave switch mood { // Choose from among our pre-populated histograms case .newWave: sourceHistogramToSpecify = sourceImageHistogramNewWave case .berlin: sourceHistogramToSpecify = sourceImageHistogramBerlin case .oldFilm: sourceHistogramToSpecify = sourceImageHistogramOldFilm case .sunset: sourceHistogramToSpecify = sourceImageHistogramSunset case .badEnergy: sourceHistogramToSpecify = sourceImageHistogramBadEnergy case .beyondTheBeyond: sourceHistogramToSpecify = sourceImageHistogramBeyondTheBeyond case .drama: sourceHistogramToSpecify = sourceImageHistogramDrama case .bypass: return } if let sourceImageHistogram = sourceHistogramToSpecify { destinationBuffer.deinterleave( destination: histogramBuffer) histogramBuffer.specifyHistogram( sourceImageHistogram, destination: histogramBuffer) histogramBuffer.interleave( destination: destinationBuffer) } } private func getHistogram(for image: NSImage) -\u0026gt; vImage.PixelBuffer.HistogramFFF? { // Extract histogram from image let sourceImageHistogramBuffer = vImage.PixelBuffer( size: pixelBufferSize, pixelFormat: vImage.PlanarFx3.self) var proposedRect = NSRect( origin: CGPoint(x: 0.0, y: 0.0), size: CGSize(width: image.size.width, height: image.size.height)) guard let cgImage = image.cgImage(forProposedRect: \u0026amp;proposedRect, context: nil, hints: nil) else { logger.error(\u0026#34;Couldn\u0026#39;t get cgImage from \\(image), returning.\u0026#34;) return nil } let bytesPerPixel = cgImage.bitsPerPixel / cgImage.bitsPerComponent let destBytesPerRow = outputWidth * bytesPerPixel guard let colorSpace = cgImage.colorSpace, let context = CGContext( data: nil, width: outputWidth, height: outputHeight, bitsPerComponent: cgImage.bitsPerComponent, bytesPerRow: destBytesPerRow, space: colorSpace, bitmapInfo: cgImage.alphaInfo.rawValue) else { logger.error(\u0026#34;Problem setting up cgImage resize, returning.\u0026#34;) return nil } context.interpolationQuality = .none context.draw(cgImage, in: CGRect(x: 0, y: 0, width: outputWidth, height: outputHeight)) guard let resizedCGImage = context.makeImage() else { logger.error(\u0026#34;Couldn\u0026#39;t resize cgImage for histogram, returning.\u0026#34;) return nil } let pixelFormat = vImage.InterleavedFx3.self let sourceImageBuffer: vImage.PixelBuffer\u0026lt;vImage.InterleavedFx3\u0026gt;? do { sourceImageBuffer = try vImage.PixelBuffer( cgImage: resizedCGImage, cgImageFormat: \u0026amp;cgImageFormat, pixelFormat: pixelFormat) if let sourceImageBuffer = sourceImageBuffer { sourceImageBuffer.deinterleave(destination: sourceImageHistogramBuffer) return sourceImageHistogramBuffer.histogram(binCount: histogramBinCount) } else { logger.error(\u0026#34;Source image buffer was nil, returning.\u0026#34;) return nil } } catch { logger.error(\u0026#34;Error creating source image buffer: \\(error)\u0026#34;) return nil } } } Well, this is a very exciting class. Let me talk a little bit about the goals, and then I will describe what is going on in here.\nSo, what I wanted to do was to create a very filmic, very color-pushed visual effect with noise, maybe a little like an Anton Corbijn image in color, but much more unreal. I was thinking about how you could extract a histogram from a still image and apply it to another still in Accelerate vImage, and decided that I would like to combine this with the new vImage Pixel Buffers functions by taking several still images I thought were interesting, extracting their histograms at initialization, keeping the histograms in variables, and then applying them (specifying them) to our streaming video frames in realtime.\nI also need to apply some noise, both for stylistic reasons but also because it is the only way that these forced histograms can dither enough to draw gradients in shot backgrounds – otherwise they would just draw solid color blocks every time there is a \u0026ldquo;missing\u0026rdquo; color in the histogram that would otherwise be needed to bridge two colors in a gradient. To get my noise, I am referencing Apple\u0026rsquo;s fantastic sample app Using vImage Pixel Buffers to Generate Video Effects, with a huge caveat, which is: I don\u0026rsquo;t think we should ever be using a function like this to generate fresh noise at every buffer callback. I\u0026rsquo;m sure that Apple doesn\u0026rsquo;t intend for developers to do that; they are just showing how performant it is, and how to implement it. But, if someone were to do that in a camera, it would use a lot of fuel doing something that human perception can\u0026rsquo;t memorize in a detailed way over time, so we should be fooling those humans with something that just looks like fresh noise.\nThis means, much like with my histograms, I am filling up a set number of buffers with generated vImage noise at initialization, and applying them to the realtime video stream buffers in a way that makes them look random: I go forward through the array, I go backwards through the array, then I shuffle the array, then I do it again. This prevents noise pattern recognition, where the sequence of noise appears to do a preset \u0026ldquo;dance\u0026rdquo; at a regular interval. But it means that whether you talk for a minute or an hour, the same amount of energy was spent on generating the noise buffers. If you think it\u0026rsquo;s too frugal and you can see it when you pixel-peep, try a larger number. I would like to find a way to optimize my histogram specification more as well, because without the live noise generation, that part is now the biggest resource user.\nLast, because it is a crowd-pleaser, if my Twitter feed is anything to go by, I included a very small amount of temporal blur just to make it frea\u0026ndash; uh, to create a filmic motion blur? Sure, that\u0026rsquo;s it. Unlike the noise function, the temporal blur is very resource-frugal and doesn\u0026rsquo;t need optimization or tricks, so it can just be run similarly to how Apple does it in their example (but much like my noise constants, I am using a lot less of the temporal blur because it is A Lot, and it can get disturbing with a low color palette and low framerate).\nThe last thing affecting the look of the video feed is that we turn the framerate down to 24.\nThis is not a prettification filter; I promised in the last post that we were going to get satisfyingly weird in this post and I keep my promises. This is, like, for Contrapoints to use while addressing Congress via video feed on a giant screen, or for Blixa Bargeld to use while making the school carpool arrangements.\nIt\u0026rsquo;s also a nice one for a tutorial because you can easily try it out with different jpegs of your own, change the noise and temporal constants, etc.\nThere are seven images, representing seven \u0026ldquo;moods\u0026rdquo; or looks.\nThe basic process of the filters are similar. Buffers are specified at initialization with the vImage Pixel Buffer buffer type they are going to need in the callback. We have a converter we will be able to use to take our camera feed CVPixelBuffer and turn it into our vImage Pixel Buffer destinationBuffer, which we can then do our vImage operations on. Each of the three effect passes starts and ends with a prepared destinationBuffer, and when we\u0026rsquo;re done, we will convert destinationBuffer back into a prebaked CVPixelBuffer and send() it out to our stream.\nReady? Let\u0026rsquo;s plug it into ExtensionStreamSource. Give ExtensionStreamSource some new variables:\nlet effects = Effects() var destinationCVPixelBuffer: CVPixelBuffer? var deviceSource: ExtensionDeviceSource? and add these lines to its init(localizedName: String, streamID: UUID, streamFormat: CMIOExtensionStreamFormat, device: CMIOExtensionDevice) right under super.init() to set up a destinationCVPixelBuffer in advance, to receive the post-effects frames before dispatch to the stream output:\n self.deviceSource = device.source as? ExtensionDeviceSource guard let deviceSource = deviceSource else { logger.error(\u0026#34;No device source, returning\u0026#34;) return } let pixelBufferAttributes: NSDictionary = [ kCVPixelBufferWidthKey: outputWidth, kCVPixelBufferHeightKey: outputHeight, kCVPixelBufferPixelFormatTypeKey: deviceSource._videoDescription.mediaSubType, kCVPixelBufferIOSurfacePropertiesKey: [:], ] let result = CVPixelBufferCreate(kCFAllocatorDefault, outputWidth, outputHeight, kCVPixelFormatType_422YpCbCr8, pixelBufferAttributes as CFDictionary, \u0026amp;destinationCVPixelBuffer) if result != 0 { logger.error(\u0026#34;Couldn\u0026#39;t create destination buffer, returning\u0026#34;) return } Replace our good old sampleBufferDelegate callback:\nfunc captureOutput(_: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from _: AVCaptureConnection)\nwith this version that uses our effect buffers and effects:\nfunc captureOutput(_: AVCaptureOutput, // Callback for sampleBuffers of captured video, which we apply our effects to in realtime didOutput sampleBuffer: CMSampleBuffer, from _: AVCaptureConnection) { guard let pixelBuffer = sampleBuffer.imageBuffer, let deviceSource = deviceSource, let destinationCVPixelBuffer = destinationCVPixelBuffer else { logger.debug(\u0026#34;Nothing to do in sampleBuffer callback, returning.\u0026#34;) return } CVPixelBufferLockBaseAddress( pixelBuffer, CVPixelBufferLockFlags.readOnly) effects.populateDestinationBuffer(pixelBuffer: pixelBuffer) if deviceSource.mood != .bypass { effects.artFilm(forMood: deviceSource.mood) } CVPixelBufferUnlockBaseAddress( pixelBuffer, CVPixelBufferLockFlags.readOnly) var err: OSStatus = 0 var sbuf: CMSampleBuffer! var timingInfo = CMSampleTimingInfo() timingInfo.presentationTimeStamp = CMClockGetTime(CMClockGetHostTimeClock()) CVPixelBufferLockBaseAddress(destinationCVPixelBuffer, CVPixelBufferLockFlags(rawValue: 0)) do { try effects.destinationBuffer.copy( to: destinationCVPixelBuffer, cvImageFormat: effects.cvImageFormat, cgImageFormat: effects.cgImageFormat) } catch { logger.error(\u0026#34;Copying to the destinationBuffer failed.\u0026#34;) } CVPixelBufferUnlockBaseAddress(destinationCVPixelBuffer, CVPixelBufferLockFlags(rawValue: 0)) var formatDescription: CMFormatDescription? CMVideoFormatDescriptionCreateForImageBuffer( allocator: kCFAllocatorDefault, imageBuffer: destinationCVPixelBuffer, formatDescriptionOut: \u0026amp;formatDescription) err = CMSampleBufferCreateReadyWithImageBuffer( allocator: kCFAllocatorDefault, imageBuffer: destinationCVPixelBuffer, formatDescription: formatDescription!, sampleTiming: \u0026amp;timingInfo, sampleBufferOut: \u0026amp;sbuf) // CVPixelBuffer into CMSampleBuffer for streaming out if err == 0 { if deviceSource._isExtension { // If I\u0026#39;m the extension, send to output stream stream.send( sbuf, discontinuity: [], hostTimeInNanoseconds: UInt64(timingInfo.presentationTimeStamp.seconds * Double(NSEC_PER_SEC))) } else { deviceSource.extensionDeviceSourceDelegate? .bufferReceived(sbuf) // If I\u0026#39;m the end to end testing app, send to delegate method. } } else { logger.error(\u0026#34;Error in stream: \\(err)\u0026#34;) } } You can now completely remove the following functions and variables from ExtensionProvider.swift:\nvar imageIsClean = true func pixelBufferFromImage(_ image: CGImage) -\u0026gt; CVPixelBuffer? private var _whiteStripeStartRow: UInt32 = 0 private var _whiteStripeIsAscending: Bool = false func startStreaming() func stopStreaming()  private var _streamingCounter: UInt32 = 0 private var _timer: DispatchSourceTimer? private let _timerQueue = DispatchQueue(label: \u0026quot;timerQueue\u0026quot;, qos: .userInteractive, attributes: [], autoreleaseFrequency: .workItem, target: .global(qos: .userInteractive))\nThat\u0026rsquo;s it for the extension. You should be able to run it in the end-to-end app now and try out its filters. The only thing we\u0026rsquo;re missing is a video monitor in the container app. As I mentioned in the first post, we are only writing UIs in SwiftUI in these posts, and not using NSViewRepresentable, so let\u0026rsquo;s reuse our CaptureSessionManager in combination with our IOSurface-\u0026gt;CGImage approach from the end-to-end app to give ourselves a feed of OffcutsCam when the extension is installed.\nAdd this class to the container app\u0026rsquo;s ContentView.swift:\n // MARK: - OutputImageManager class OutputImageManager: NSObject, AVCaptureVideoDataOutputSampleBufferDelegate, ObservableObject { @Published var videoExtensionStreamOutputImage: CGImage? let noVideoImage: CGImage = NSImage( systemSymbolName: \u0026#34;video.slash\u0026#34;, accessibilityDescription: \u0026#34;Image to indicate no video feed available\u0026#34; )!.cgImage(forProposedRect: nil, context: nil, hints: nil)! // OK to fail if this isn\u0026#39;t available. func captureOutput(_: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from _: AVCaptureConnection) { autoreleasepool { guard let cvImageBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) else { logger.debug(\u0026#34;Couldn\u0026#39;t get image buffer, returning.\u0026#34;) return } guard let ioSurface = CVPixelBufferGetIOSurface(cvImageBuffer) else { logger.debug(\u0026#34;Pixel buffer had no IOSurface\u0026#34;) // The camera uses IOSurface so we want image to break if there is none. return } let ciImage = CIImage(ioSurface: ioSurface.takeUnretainedValue()) .oriented(.upMirrored) // Cameras show the user a mirrored image, the other end of the conversation an unmirrored image. let context = CIContext(options: nil) guard let cgImage = context .createCGImage(ciImage, from: ciImage.extent) else { return } DispatchQueue.main.async { self.videoExtensionStreamOutputImage = cgImage } } } } This will receive the output buffers from a properly-configured AVCaptureSession and convert them to a CGImage. Change the init() of ContentView to this:\n init(systemExtensionRequestManager: SystemExtensionRequestManager, propertyManager: CustomPropertyManager, outputImageManager: OutputImageManager) { self.propertyManager = propertyManager self.systemExtensionRequestManager = systemExtensionRequestManager self.outputImageManager = outputImageManager effect = moods.firstIndex(of: propertyManager.effect) ?? 0 captureSessionManager = CaptureSessionManager(capturingOffcutsCam: true) if captureSessionManager.configured == true, captureSessionManager.captureSession.isRunning == false { captureSessionManager.captureSession.startRunning() captureSessionManager.videoOutput?.setSampleBufferDelegate(outputImageManager, queue: captureSessionManager.dataOutputQueue) } else { logger.error(\u0026#34;Couldn\u0026#39;t start capture session\u0026#34;) } } and add these variables to ContentView:\n@ObservedObject var outputImageManager: OutputImageManager var captureSessionManager: CaptureSessionManager in the View of ContentView, add this image:\n Image( self.outputImageManager .videoExtensionStreamOutputImage ?? self.outputImageManager .noVideoImage, scale: 1.0, label: Text(\u0026#34;Video Feed\u0026#34;) ) Wherever that ContentView is invoked, change the call to this:\nContentView(systemExtensionRequestManager: SystemExtensionRequestManager(logText: \u0026#34;\u0026#34;), propertyManager: CustomPropertyManager(), outputImageManager: OutputImageManager()) Let\u0026rsquo;s add this to both our VStacks in the ContentView of the container app and the end-to-end testing app so our layout is nicer:\n.frame(alignment: .top) Spacer() And let\u0026rsquo;s add this frame everywhere those ContentViews are created from:\n.frame(minWidth: 1280, maxWidth: 1360, minHeight: 900, maxHeight: 940) And now you should be able to see your installed extension content in an image at the top of your container app. When installing the extension for the first time, it is necessary to quit and restart the container app before this will work.\nI encountered one persistent bug in this phase of this post, which is that sometimes, the /Applications version of OffcutsCam.app would complain when asked to install the extension that it had an invalid codesign. In these cases, it was necessary to select Xcode-\u0026gt;Product-\u0026gt;Show Build Folder in Finder and move the copy of OffcutsCam.app in there to /Applications manually, at which point the error stopped.\nI encountered a milder bug (but still something that would be very stressful if I were discovering this the hard way via a tutorial not working) which was that I had one installation where the effects didn\u0026rsquo;t work in the container app\u0026lt;-\u0026gt;extension interaction but they did work in the end-to-end app, which self-healed after a second install, restart, reinstall. Well, we know that we\u0026rsquo;re using betas and some mysteries are part of that.\nThat\u0026rsquo;s everything! This has been quite a journey. I hope you feel well-set-up to start experimenting with your own effects and creative camera experiences. If you\u0026rsquo;ve had any trouble, compare against my version on Github and I\u0026rsquo;m sure you\u0026rsquo;ll find the issue in no time. Have fun!\nExtro #  ✂ - - Non-code thoughts, please stop reading here for the code-only experience - - ✂\nExtro will follow up in a few days after publication, as usual.\n"},{"id":1,"href":"/posts/core-media-io-camera-extensions-part-two/","title":"Getting To Grips With The Core Media IO Camera Extension Part 2 of 3: Interprocess Communication and Debugging","section":"Posts","content":"Getting To Grips With The Core Media IO Camera Extension, a 3 part series. #  Part 2 of 3: Creating a useful software Core Media Camera Extension, interprocess communication, and painless debugging. #  August 18th, 2022 To the code →\n View project on Github #  Welcome to the second in a series of three posts about the Core Media IO Camera Extension.\nPlease take a look at the previous post, \u0026ldquo;The Basics\u0026rdquo;, to learn what the series is about, its prerequisites, etc, and to understand the project so far.\nThe second post, this one, is about creating a useful software CMIO Camera Extension with communication between a configuration app and an extension, and painless extension debugging,\nThe last will be about bringing it all together by building a creative camera with realtime effects processing using vImage Pixel Buffers, which can use the Continuity Camera Webcam.\nLet\u0026rsquo;s jump in. In part 1, we created a known-working basic software CMIO Camera Extension camera with a container app which could install it, that we could use in FaceTime in order to watch a white stripe move up and down. You can view or clone my version of it, keeping in mind that you will need to change all references to my organization ID (com.politepix) and my team ID to match your own, or it definitely won\u0026rsquo;t work. Ideally you have your own that you made in the previous post and it is still working, so we can continue with it here.\nWe are going to examine three more fairly buttoned-down topics today that will leave us prepared to get satisfyingly weird in part 3, coming up next. They are:\n Interprocess communication between a container app and the extension it installs (i.e. how to turn the installer app into a configuration app), Creating a simple software CMIO Camera Extension that can do something slightly more useful than the white line, How to happily debug your extension, so you don\u0026rsquo;t have to see your own debug-hell facial expression on the screen every time you test a camera feed.  Pick up where we left off #  As we\u0026rsquo;ve seen, Core Media IO Camera Extensions use a container app to install an extension. What if you need to tell the extension to change something? This idea of app-to-extension communication is mentioned in the CMIO Extension WWDC22 video.\nThe container app can become a configuration app by using some form of interprocess communication. The app is a process and the extension is a separate process, so their code doesn\u0026rsquo;t speak to each other directly, and in fact, there is no reason to assume they will both be an active process simultaneously — the user can effectively choose to load one process, the other, both, or none. The WWDC video states that the extension is run by a different user role account than the user\u0026rsquo;s account, for security. They are genuinely separate.\nWhat I really wanted to show you was how to share an App Group UserDefaults between the container app and the extension, and then use CFNotification to message between them. This covers many cases and it also encourages an uncoupled communication design.\nThe docs indicate that using UserDefaults initialized with UserDefaults(suiteName:) in a shared App Group container is supported, and we have an App Group, and I have seen it work in some OS versions. But, after a bit of time examining my results with those UserDefaults across different betas and current OS versions, I have the feeling that how it\u0026rsquo;s \u0026ldquo;supposed\u0026rdquo; to work is in flux right now, and I suspect I could be sharing code with a very short expiration date.\nThen I thought I\u0026rsquo;d show you some simple XPC instead, but it currently raises the same \u0026ldquo;is this supposed to be like this?\u0026rdquo; questions as UserDefaults. Well, this is a new way of distributing these extensions, so we can expect some bumps and changes.\nAnyway. Let us not chase waterfalls; let us stick to the CFNotifications we are used to. I think I\u0026rsquo;ll come back and revise this to include sharing data once Ventura has settled down into later betas. In part 3, we\u0026rsquo;ll add an additional form of configuration via custom properties, but right now we\u0026rsquo;ll keep it simple.\nNotifications (and Logger) #  Let\u0026rsquo;s add some notification code that lets us communicate from the container app to the extension without ambiguity.\nFirst, let\u0026rsquo;s add a file we will share with both targets. Xcode-\u0026gt;File-\u0026gt;New-File-\u0026gt;Swift File and call it NotificationName.swift.\n  When creating it, make sure to add it to both app and extension targets. This should be its contents (but with your team ID and organization ID):\nimport Foundation enum NotificationName: String, CaseIterable { case changeImage = \u0026#34;Z39BRGKSRW.com.politepix.OffcutsCam.changeImage\u0026#34; // add more cases later like this one, changing the case `changeImage` and the substring \u0026#34;changeImage\u0026#34; to a different notification name you want to pass. } Next, add this import to ContentView.swift:\nimport OSLog\nRight under it, we\u0026rsquo;ll create a unified logging API logger (please also check out this indispensable post about making the system log work for you). In a project that was less of a demonstration, there are several ways we could do this that would be more design-correct. But I\u0026rsquo;d like to show this with minimal indirection, so we\u0026rsquo;ll create a global here in the file where we\u0026rsquo;re doing the work:\nlet logger = Logger(subsystem: \u0026quot;com.politepix.offcutscam\u0026quot;, category: \u0026quot;Application\u0026quot;)\nAdd this new function to your SystemExtensionRequestManager class in ContentView.swift of the container app, substituting your own IDs where you see mine:\nfunc postNotification(named notificationName: NotificationName) { logger.debug(\u0026#34;Posting notification \\(notificationName.rawValue) from container app\u0026#34;) CFNotificationCenterPostNotification( CFNotificationCenterGetDarwinNotifyCenter(), CFNotificationName(notificationName.rawValue as NSString), nil, nil, true ) } By using our Logger instead of print(...), we can see our logging in Console.app, which is going to be one of the three legs of our extension debugging strategy. We\u0026rsquo;re using Logger.debug as our logging level because we don\u0026rsquo;t want this logging to persist in the OS. Other options would be info (doesn\u0026rsquo;t persist unless implicated in an error) or error (persists), and more.\nAdd this UI to the top of ContentView\u0026rsquo;s view:\nButton(action: { systemExtensionRequestManager .postNotification( named: NotificationName.changeImage ) }) { Text(\u0026#34;Change Image\u0026#34;) } .padding() If you build and run the container app now, you will see a button, and if you click it, it will post a notification and you should be able to see the logger.debug(...) line logged in Xcode. You can see the beginning of a design, where we add new notifications in NotificationName and call them, due to interactions, with postNotification(named:) using the cases of the NotificationName enum.\nNow we will set up receiving notifications in the extension.\nFirst, add a logger to the top of the file:\nlet logger = Logger(subsystem: \u0026quot;com.politepix.offcutscam\u0026quot;, category: \u0026quot;Extension\u0026quot;)\nYou can see that we have a different category for the extension, which will let us view both app and extension logging under the shared subsystem in Console.app, but view either exclusively using the category.\nAdd the following to your extension\u0026rsquo;s ExtensionProviderSource in ExtensionProvider.swift:\n private let notificationCenter = CFNotificationCenterGetDarwinNotifyCenter() private var notificationListenerStarted = false In ExtensionProviderSource init() function, add this line right after super.init():\n startNotificationListeners() and after the init, in the same module, add these new functions:\ndeinit { stopNotificationListeners() } private func notificationReceived(notificationName: String) { guard let name = NotificationName(rawValue: notificationName) else { return } switch name { case .changeImage: logger.debug(\u0026#34;The camera extension has received a notification\u0026#34;) logger.debug(\u0026#34;The notification is: \\(name.rawValue)\u0026#34;) } // This switch will complain about missing cases as you add new notifications to the NotificationName enum. The system works! } private func startNotificationListeners() { for notificationName in NotificationName.allCases { let observer = UnsafeRawPointer(Unmanaged.passUnretained(self).toOpaque()) CFNotificationCenterAddObserver(CFNotificationCenterGetDarwinNotifyCenter(), observer, { _, observer, name, _, _ in if let observer = observer, let name = name { let extensionProviderSourceSelf = Unmanaged\u0026lt;ExtensionProviderSource\u0026gt;.fromOpaque(observer).takeUnretainedValue() extensionProviderSourceSelf.notificationReceived(notificationName: name.rawValue as String) } }, notificationName.rawValue as CFString, nil, .deliverImmediately) } } private func stopNotificationListeners() { if notificationListenerStarted { CFNotificationCenterRemoveEveryObserver(notificationCenter, Unmanaged.passRetained(self) .toOpaque()) notificationListenerStarted = false } } This shows that we are going to listen for a notification that the app is sending to CFNotificationCenterGetDarwinNotifyCenter and get it into a switch case of our NotificationName enum.\nIf we could see extension logging in Xcode, we would run our extension and press the container app button to see if communication is working, but we can\u0026rsquo;t log the extension output to Xcode.\nIt is necessary to install and activate the extension and open Console.app, and that will allow us to view all of our unified logging output that we wrote with our logger in the console.\nI mentioned at the end of part 1 that we were going to get around the necessity to constantly reboot in order to debug, and we are, but to get there we\u0026rsquo;re going to need to do a reboot, so we can first see what it is like to debug a camera extension the hard way.\nsystemextensionsctl #  There is a handy CLI tool called systemextensionsctl and let\u0026rsquo;s open Terminal.app and use it to check the current status of our extension. Run systemextensionsctl list to see the current state of the user-installed system extensions on your system. If you installed your extension and never uninstalled it, it will look something like this:\n*\t*\tZ39BRGKSRW\tcom.politepix.OffcutsCam.Extension (1.0/1)\tExtension\t[activated enabled]\nIf you haven\u0026rsquo;t already done so, build and run /Applications/OffcutsCam.app and press the Uninstall button to uninstall your previous version of the extension. Now systemextensionsctl list will look like this:\n\tZ39BRGKSRW\tcom.politepix.OffcutsCam.Extension (1.0/1)\tExtension\t[terminated waiting to uninstall on reboot]\nIt is telling you that the extension isn\u0026rsquo;t going to be fully uninstalled until after a reboot – there are no live replacements of an extension process in a single user session. So reboot. After your system comes back up, build and run the container app in Xcode, and install your new extension version in the container app.\nViewing our logging in Console.app #  After you have successfully installed it, open Console.app and click Start Streaming. You will see a lot of messages. In the search filter field in the upper right, enter com.politepix.offcutscam (but with your organization ID) so we can look for messages about or from our extension and app. There is a dropdown in the filter field next to your search token, and let\u0026rsquo;s set it to Subsystem, so we are only receiving output from our logger.\nNow that we have quieted things down, we will go to Console.app-\u0026gt;Action-\u0026gt;View Debug Messages and check that so we can see our debug-level logging. You will probably want to turn this off after you\u0026rsquo;re done debugging your extension because it\u0026rsquo;s intolerably noisy if you ever need to look at OS events.\nNext, we want to actually load our extension, so let\u0026rsquo;s open FaceTime and select our camera. The extension process isn\u0026rsquo;t going to load unless an app uses the camera.\nNow that the camera extension is loaded, if we run the app and click the Change Image button, we will see our debug logging in Console.app. Since we\u0026rsquo;re filtering there on our shared subsystem, we see logging from the app and the extension. If we add a filter in the search field of Console.app for Extension and set its type to Category we will only see the extension output. If we used the category Application instead, we would only see the app output.\nTip: when you are debugging an extension by observing Console.app logging, you should occasionally filter by the extension name as a regular \u0026ldquo;contains\u0026rdquo; string and not only by the subsystem you set to catch your own debug output. Why? Because another app or process might be the one complaining about your extension process, and that may be where you get the info you need to solve a bug.\nSo, we can now do part of our debugging with Console.app, but we don\u0026rsquo;t yet know how to avoid reboots to examine code. To start dismantling the reboot industrial complex, the next tool in our toolbox will be lldb.\nDoing something useful #  But first, let\u0026rsquo;s change what the software camera does. When I think back on our new age in which we have each become our own live video broadcast studios, I think, not of the countless video meetings which went relatively smoothly, but about the few that went so strangely that they are now embedded in the culture.\nYou know, this guy trying in vain to talk about South Korea while his awesome kid rocks up like she\u0026rsquo;s going to start dropping science, or this lawyer who became, irreversibly, a cat, or the very many incidents of wardrobe malfunction.\nWhen something went wrong in the news studio, they could always cut to the \u0026ldquo;Technical Difficulties\u0026rdquo; placard in the most dire cases. We as individuals deserve at least this much dignity. We will make a \u0026ldquo;We\u0026rsquo;ve Having Technical Difficulties\u0026rdquo; software camera so that when things get weird, you too can cut away.\nDownload this image and this image, and add them to your extension target.\n  Build and run the app, and then verify that these images will be available to your extension bundle by opening Terminal.app and running open /Applications/OffcutsCam.app/Contents/Library/SystemExtensions/com.politepix.OffcutsCam.Extension.systemextension/Contents/Resources so you can see if they\u0026rsquo;re both in there. If not, try building the extension directly in Xcode and then going back to building and running the application (this shouldn\u0026rsquo;t be necessary, but it helped with this issue once in my experience, so give it a try). If you still don\u0026rsquo;t have the images at the necessary location, troubleshoot whether you really added them to the extension target. Once you see them, proceed.\nOK, let\u0026rsquo;s open ExtensionProvider.swift. First, change the framerate at the top of the file to 1. We are going to show a static image, so we don\u0026rsquo;t need to burn fuel by refreshing it 60x/sec:\nlet kFrameRate: Int = 1 Change the video dimensions from this: let dims = CMVideoDimensions(width: 1920, height: 1080) to this: let dims = CMVideoDimensions(width: 1080, height: 720)\nIf we recall from the explanation of ExtensionProvider.swift from part 1, the business code for creating the actual content that gets to the client app using the camera extension, in Apple\u0026rsquo;s example code, is in func startStreaming() of ExtensionDeviceSource. This is where a pixel buffer is prepared to be handed off to the object conforming to CMIOExtensionStreamSource (which is the var _streamSource in ExtensionDeviceSource).\nLet\u0026rsquo;s replace the entirety of func startStreaming() with this:\nfunc startStreaming() { guard let _ = _bufferPool else { return } guard let bundleURL = Bundle.main.url(forResource: \u0026#34;Clean\u0026#34;, withExtension: \u0026#34;jpg\u0026#34;) else { logger.debug(\u0026#34;Clean.jpg wasn\u0026#39;t found in bundle, returning.\u0026#34;) return } guard let imageData = NSData(contentsOf: bundleURL) else { logger.debug(\u0026#34;Couldn\u0026#39;t get data from image at URL, returning.\u0026#34;) return } guard let dataProvider = CGDataProvider(data: imageData) else { logger.debug(\u0026#34;Couldn\u0026#39;t get dataProvider of URL, returning\u0026#34;) return } guard let techDiffCGImage = CGImage(jpegDataProviderSource: dataProvider, decode: nil, shouldInterpolate: false, intent: .defaultIntent) else { logger.debug(\u0026#34;Couldn\u0026#39;t get CG image from dataProvider, returning\u0026#34;) return } guard let techDiffBuffer = pixelBufferFromImage(techDiffCGImage) else { logger.debug(\u0026#34;It wasn\u0026#39;t possible to get pixelBuffer from the techDiffCGImage, exiting.\u0026#34;) return } _streamingCounter += 1 _timer = DispatchSource.makeTimerSource(flags: .strict, queue: _timerQueue) _timer!.schedule(deadline: .now(), repeating: Double(1 / kFrameRate), leeway: .seconds(0)) _timer!.setEventHandler { var err: OSStatus = 0 let now = CMClockGetTime(CMClockGetHostTimeClock()) var sbuf: CMSampleBuffer! var timingInfo = CMSampleTimingInfo() timingInfo.presentationTimeStamp = CMClockGetTime(CMClockGetHostTimeClock()) var formatDescription: CMFormatDescription? let status = CMVideoFormatDescriptionCreateForImageBuffer(allocator: kCFAllocatorDefault, imageBuffer: techDiffBuffer, formatDescriptionOut: \u0026amp;formatDescription) if status != 0 { logger.debug(\u0026#34;Couldn\u0026#39;t make video format description from techDiffBuffer, exiting.\u0026#34;) } if let formatDescription = formatDescription { err = CMSampleBufferCreateReadyWithImageBuffer(allocator: kCFAllocatorDefault, imageBuffer: techDiffBuffer, formatDescription: formatDescription, sampleTiming: \u0026amp;timingInfo, sampleBufferOut: \u0026amp;sbuf) } else { logger.debug(\u0026#34;Couldn\u0026#39;t create sample buffer from techDiffBuffer, exiting.\u0026#34;) } if err == 0 { self._streamSource.stream.send(sbuf, discontinuity: [], hostTimeInNanoseconds: UInt64(timingInfo.presentationTimeStamp.seconds * Double(NSEC_PER_SEC))) } else { os_log(.info, \u0026#34;video time \\(timingInfo.presentationTimeStamp.seconds) now \\(now.seconds) err \\(err)\u0026#34;) } } _timer!.setCancelHandler {} _timer!.resume() } You can see that what we are doing here is loading in some image data and getting a CVPixelBuffer from it so we can .send(...) it to our output stream. Add the function that will get the pixel buffer with the correct attributes:\nfunc pixelBufferFromImage(_ image: CGImage) -\u0026gt; CVPixelBuffer? { let width = image.width let height = image.height let region = CGRect(x: 0, y: 0, width: width, height: height) let pixelBufferAttributes: NSDictionary = [ kCVPixelBufferWidthKey: width, kCVPixelBufferHeightKey: height, kCVPixelBufferPixelFormatTypeKey: _videoDescription.mediaSubType, kCVPixelBufferIOSurfacePropertiesKey: [:], ] var pixelBuffer: CVPixelBuffer? let result = CVPixelBufferCreate(kCFAllocatorDefault, width, height, kCVPixelFormatType_32ARGB, pixelBufferAttributes as CFDictionary, \u0026amp;pixelBuffer) guard result == kCVReturnSuccess, let pixelBuffer = pixelBuffer, let colorspace = image.colorSpace else { return nil } CVPixelBufferLockBaseAddress(pixelBuffer, CVPixelBufferLockFlags(rawValue: 0)) guard let context = CGContext(data: CVPixelBufferGetBaseAddress(pixelBuffer), width: width, height: height, bitsPerComponent: image.bitsPerComponent, bytesPerRow: image.bytesPerRow, space: colorspace, bitmapInfo: CGImageAlphaInfo.noneSkipFirst .rawValue) else { return nil } context.draw(image, in: region) CVPixelBufferUnlockBaseAddress(pixelBuffer, CVPixelBufferLockFlags(rawValue: 0)) return pixelBuffer } Build and run, and use the application to uninstall your old extension, reboot, and use your app again to install your new extension. Now we will look at how to use lldb to debug your live extension.\nOur friend LLDB #  When you run this version of the extension, by selecting it in FaceTime, you will see that it almost works, but that it doesn\u0026rsquo;t quite. The technical difficulties image is shown in the camera, but in mirror image.\nWe could make ourselves miserable by futzing with our extension code blindly and then rebooting endlessly to see the results, but we won\u0026rsquo;t. Instead, let\u0026rsquo;s attach the Xcode version of the lldb debugger to our extension while it is running in FaceTime and do some diagnosis. While FaceTime is using the extension, get its pid by opening Terminal.app and running pgrep com.politepix.OffcutsCam.Extension (replacing my organization ID with yours). Open Xcode and choose Debug-\u0026gt;Attach to Process by PID or Name… and enter that PID, and choose \u0026ldquo;root\u0026rdquo; as the account to debug with.\n  You will be asked to authenticate with an admin account and then Xcode will attach to your extension. Since you have its project open, and you built it with debug symbols, we can put breakpoints into ExtensionProvider.swift and inspect our variables directly in Xcode. Let\u0026rsquo;s do perhaps the single most important Xcode lldb trick we can learn for video extension debugging, and quicklook the contents of a pixelbuffer. Place a breakpoint right before this line in func startStreaming():\n_streamingCounter += 1\nNow go to FaceTime, select a different camera than ours, and then reselect our camera. In Xcode, your project should break at the breakpoint you set.\nCheck it out, we\u0026rsquo;re live-debugging our installed extension. Let\u0026rsquo;s open the debugger pane at the bottom of Xcode. Expand the left side if it isn\u0026rsquo;t already expanded. This is the variable viewer. You should see the object techDiffBuffer. Select it by clicking on it.\n  Now, with it selected in the variable viewer, press the space key. Whoa, you should see a whole image in that quicklook window. Xcode can show you the image contained in a pixel buffer! You can even choose to open it as an image in Preview.app. Once you\u0026rsquo;re writing effects, this is going to come in handy.\nHmm, the contents of the buffer look correct. This means that the camera output stream is being displayed in a different coordinate space after it leaves the extension.\nWe probably have to try out some code approaches here. We have two legs of our debugging available: we know how to use unified logging to see live extension console output, and we know how to set breakpoints and examine variables in the live extension in Xcode\u0026rsquo;s lldb debugger. But this is not quite enough to save us from reboot hell, because we still can\u0026rsquo;t change code and see results right after building.\nThe last and most important leg of our extension debug practice is an end-to-end testing app.\nEnding reboots using an end-to-end testing app #  Add a macOS app target to your project called OffcutsCamEndToEnd. Open this new target\u0026rsquo;s ContentView.swift and replace its entire contents with this:\n import CoreMediaIO import SwiftUI // MARK: - ContentView struct ContentView { @ObservedObject var endToEndStreamProvider: EndToEndStreamProvider } extension ContentView: View { var body: some View { VStack { Image( self.endToEndStreamProvider .videoExtensionStreamOutputImage ?? self.endToEndStreamProvider .noVideoImage, scale: 1.0, label: Text(\u0026#34;Video Feed\u0026#34;) ) } } } // MARK: - ContentView_Previews struct ContentView_Previews: PreviewProvider { static var previews: some View { ContentView(endToEndStreamProvider: EndToEndStreamProvider()) } } // MARK: - EndToEndStreamProvider class EndToEndStreamProvider: NSObject, ObservableObject, ExtensionDeviceSourceDelegate { // MARK: Lifecycle override init() { providerSource = ExtensionProviderSource(clientQueue: nil) super.init() providerSource .deviceSource = ExtensionDeviceSource(localizedName: \u0026#34;OffcutsCam\u0026#34;) providerSource.deviceSource.extensionDeviceSourceDelegate = self providerSource.deviceSource.startStreaming() } // MARK: Internal @Published var videoExtensionStreamOutputImage: CGImage? let noVideoImage : CGImage = NSImage( systemSymbolName: \u0026#34;video.slash\u0026#34;, accessibilityDescription: \u0026#34;Image to indicate no video feed available\u0026#34; )!.cgImage(forProposedRect: nil, context: nil, hints: nil)! // OK to fail if this isn\u0026#39;t available. let providerSource: ExtensionProviderSource func bufferReceived(_ buffer: CMSampleBuffer) { guard let cvImageBuffer = CMSampleBufferGetImageBuffer(buffer) else { print(\u0026#34;Couldn\u0026#39;t get image buffer, returning.\u0026#34;) return } guard let ioSurface = CVPixelBufferGetIOSurface(cvImageBuffer) else { print(\u0026#34;Pixel buffer had no IOSurface\u0026#34;) // The camera uses IOSurface so we want image to break if there is none. return } let ciImage = CIImage(ioSurface: ioSurface.takeUnretainedValue()) .oriented(.upMirrored) let context = CIContext(options: nil) guard let cgImage = context .createCGImage(ciImage, from: ciImage.extent) else { return } DispatchQueue.main.async { self.videoExtensionStreamOutputImage = cgImage } } } Change anything that loads ContentView() to load it like this:\nContentView(endToEndStreamProvider: EndToEndStreamProvider()) You can see that we are trying to start ExtensionProvider as it is started in the extension\u0026rsquo;s main() function, but attempting to substitute for the parts we have no access to.\nIn the navigator, select the files Clean.jpg, Dirty.jpg, NotificationName.swift, and ExtensionProvider.swift and add them to the new target:\n  In ExtensionProvider.swift, make the following small changes so we can receive the camera\u0026rsquo;s buffers in our end-to-end app.\nAdd this protocol to the top of the file:\nprotocol ExtensionDeviceSourceDelegate: NSObject { func bufferReceived(_ buffer: CMSampleBuffer) } and add this delegate and boolean to your class ExtensionDeviceSource:\npublic weak var extensionDeviceSourceDelegate: ExtensionDeviceSourceDelegate? private var _isExtension: Bool = true Add this to init(localizedName: String) of ExtensionDeviceSource right after super.init():\nguard let bundleID = Bundle.main.bundleIdentifier else { return } if bundleID.contains(\u0026#34;EndToEnd\u0026#34;) { _isExtension = false } and change this:\n self._streamSource.stream.send(sbuf, discontinuity: [], hostTimeInNanoseconds: UInt64(timingInfo .presentationTimeStamp .seconds * Double(NSEC_PER_SEC))) to this:\nif self._isExtension { // If I\u0026#39;m the extension, send to output stream self._streamSource.stream.send(sbuf, discontinuity: [], hostTimeInNanoseconds: UInt64(timingInfo.presentationTimeStamp.seconds * Double(NSEC_PER_SEC))) } else { self.extensionDeviceSourceDelegate?.bufferReceived(sbuf) // If I\u0026#39;m the end-to-end testing app, send to delegate method. } Now we can send the buffers to the output stream when the ExtensionProvider is in a camera extension, and send them to the end-to-end app when we run it directly. We want to attempt to make the end-to-end app behave as similarly as possible to the CMIO Camera Extension output stream, so we can use it for as much video debugging as possible. If you now build and run the end-to-end app, you should see it break on our last breakpoint we sent in ExtensionProvider.swift.\nRemove the breakpoint and continue. You should see our image in the app, flipped on its vertical axis just like in our video feed. Now we can debug the extension code as if it were an app.\nI am going to add a transform to the ExtensionDeviceSource function func pixelBufferFromImage(_ image: CGImage) -\u0026gt; CVPixelBuffer?. I will replace the line context.draw(image, in: region) with this:\nvar transform = CGAffineTransform(scaleX: -1, y: 1) // Flip on vertical axis transform = transform.translatedBy(x: -CGFloat(image.width), y: 0) context.concatenate(transform) context.draw(image, in: region) Now when I run the end-to-end testing app, I see my image oriented correctly. You can build and run a new OffcutsCam.app and install the fixed extension since image debugging is done for now, and you should see it working.\nThe end-to-end app tool has some maintenance attached to it. The goal is that it should do very little, but what it does should approximate what the system does with the camera\u0026rsquo;s output stream, with the same requirements and behaviors. That\u0026rsquo;s why it creates its image from an IOSurface and not from the CVPixelBuffer, for instance, because if the CVPixelBuffer was lacking an IOSurface, and that is possible in a valid CVPixelBuffer, we could successfully make an image from it in our end-to-end testing app, but we would not get an image in FaceTime when using our camera, because the camera system passes IOSurface for efficiency.\nMy first expected maintenance of this tool: I suspect my vertical-axis flipping in the end-to-end app isn\u0026rsquo;t correct for pixel buffers that originate from AVCaptureSession video, so I foresee a future improvement in which, instead of flipping the image, it complains when a pixel buffer was created with the wrong format or properties that would result in being in the wrong coordinate space once it gets to the camera system. But, it\u0026rsquo;s a lot easier to do that research and experimentation now.\nChanging the extension by using the container app, finally #  Now we can finally have the app talk to the extension and make a change in it. We will test this in the end-to-end testing app, and once it is working, we\u0026rsquo;ll reboot and view it in the actual extension.\nIn ExtensionProvider.swift, go to private func notificationReceived(notificationName: String) and, under case .changeImage:, replace the two logging lines with this:\nself.deviceSource.imageIsClean.toggle() logger.debug(\u0026#34;The camera extension has received a notification\u0026#34;) logger.debug(\u0026#34;The notification is: \\(name.rawValue)\u0026#34;) self.deviceSource.stopStreaming() self.deviceSource.startStreaming() Give ExtensionDeviceSource this var: var imageIsClean = true\nAnd replace these lines in func startStreaming():\nguard let bundleURL = Bundle.main.url(forResource: \u0026#34;Clean\u0026#34;, withExtension: \u0026#34;jpg\u0026#34;) else { logger.debug(\u0026#34;Clean.jpg wasn\u0026#39;t found in bundle, returning.\u0026#34;) return } with these:\nlet Filename = imageIsClean ? \u0026#34;Clean\u0026#34; : \u0026#34;Dirty\u0026#34; guard let bundleURL = Bundle.main.url(forResource: Filename, withExtension: \u0026#34;jpg\u0026#34;) else { logger.debug(\u0026#34;Clean.jpg wasn\u0026#39;t found in bundle, returning.\u0026#34;) return } Now, when we run the end-to-end testing app and run OffcutsCam.app, clicking Change Image shows us a change of image. We can toggle between a dirty and a clean version of the technical difficulties placard.\nReboot and install the extension once it is debugged #  Since we seem to have a working implementation, let\u0026rsquo;s build and run a new OffcutsCam.app (not the end-to-end testing app), uninstall the old extension, reboot, and install a new extension.\nIt works! Well, mine works; yours might need some more debugging. Good thing you know how to use unified logging, Xcode lldb, and an end-to-end testing app to debug your CMIO Camera Extension with (almost) no reboots. You can take a look at my working version on Github, remembering to change all incidences of team ID and organization ID from mine to yours.\nNow that we know how to make a software camera extension with communication from its container app, and we know how to debug, next up will be fun stuff, in part 3 of my CMIO Camera Extension Series: building a creative camera with realtime effects processing using vImage Pixel Buffers, which can use the Continuity Camera Webcam.\nExtro #  ✂ - - Non-code thoughts, please stop reading here for the code-only experience - - ✂\nMore video debugging, more Epictetus. Excerpt:\n\u0026ldquo;It is the part of an uneducated person to blame others where he himself fares ill; to blame himself is the part of one whose education has begun; to blame neither another nor his own self is the part of one whose education is already complete.\u0026rdquo;\nHe was a stoic, so what he probably meant was that we can\u0026rsquo;t control external things, only what we make of them (and education is learning this fact). Meaning, at the stage in which we have no control over what we make of the impressions in our brains, and too much belief in the idea that people are directly in control of external events, we blame the person outside of ourselves closest to a negative outcome. With a little control over ourselves, we blame ourselves for not having enough control over ourselves. When we truly understand that the only influence we can claim to reliably have over reality is the reality in our interpretation of our perceptions, we blame no one.\nI am not a stoic, so my understanding is: we first react to injury without any compassion, and later we are able to demonstrate compassion for another. The next stage would be to include ourselves and another in the same compassion, where it isn\u0026rsquo;t a demonstration.\nSomeone else might say \u0026ldquo;First you think those perspectives are opposite, then you think they\u0026rsquo;re just orthogonal, then you notice that they overlap\u0026hellip;\u0026rdquo;\n"},{"id":2,"href":"/posts/core-media-io-camera-extensions-part-one/","title":"Getting To Grips With The Core Media IO Camera Extension Part 1 of 3: The Basics","section":"Posts","content":"Getting To Grips With The Core Media IO Camera Extension, a 3 part series. #  Part 1 of 3: The Basics: creating an installable Core Media Camera Extension and its container app. #  August 11th, 2022 To the code →\n View project on Github #  Welcome to the first in a series of three posts about the Core Media IO Camera Extension. I was delighted by the Create camera extensions with Core Media IO  WWDC22 video, which is about extensions which can present a system camera for use in any camera-supporting app such as FaceTime, including creative cameras that can take in a feed from an existing camera such as a Continuity Camera Webcam and add effects to it. Still, I wished that it had sample code for the two types of cameras that it discussed, software camera and creative camera with configuration app.\nI wrote myself some sample code, and now I would like to share it, and explain a little about how it works.\nThere will be three posts in the series:\nThe first post, this one, is about getting the basics working from the start,\nthe second is about creating a useful software CMIO Camera Extension with communication between a configuration app and an extension and painless extension debugging,\nand the last is about bringing it all together by building a creative camera with realtime effects processing using vImage Pixel Buffers, which can use the Continuity Camera Webcam.\nEach will build on the previous post. SwiftUI will be the only interface framework used, and there will be no NSViewRepresentable, which is going to get spicy in the final entry when we\u0026rsquo;re observing a camera feed in the configuration app.\nPrerequisites #  Much of this will work in macOS 12.3 and later with Xcode 13, so my sample apps for part 1 and 2 will build, run and work with them. But, the end result will explore beta APIs, so this series as a whole has been written for the betas. To run all of the code in all three parts:\n Ventura beta 5 or later Xcode 14 beta 5 or later An iPhone XS or later running iOS 16 beta 5 or later if you want to test a Continuity Camera Webcam camera extension in later parts of this series An Apple Developer account so you have a team ID and can codesign, which is already working correctly on your system when you sign other apps. If you\u0026rsquo;ve had some codesign weirdness you\u0026rsquo;ve been ignoring, this isn\u0026rsquo;t the project for working through it, trust me. It is certainly a good idea to have looked at the WWDC video and the docs, but it isn\u0026rsquo;t a requirement.  Let\u0026rsquo;s jump in. Here are the instructions for creating a container app and installable CMIO Camera Extension which is a basic software camera (you can also clone mine from Github or refer to it as you go):\nProject configuration #   Create a new project of type \u0026ldquo;macOS App\u0026rdquo;. Title it \u0026ldquo;OffcutsCam\u0026rdquo;, select your team and organization identifier. Your organization identifier will be different than mine.     Go to the app target, select Signing \u0026amp; Capabilities, and remove \u0026ldquo;Hardened Runtime\u0026rdquo;. Under App Sandbox, check \u0026ldquo;Camera\u0026rdquo; and, for debugging purposes only, set \u0026ldquo;User Selected File\u0026rdquo; to \u0026ldquo;Read Only\u0026rdquo;.     Click the \u0026ldquo;+\u0026rdquo; button at the top left to add a capability. Add \u0026ldquo;System Extension\u0026rdquo;.    Now add a target to your project (File-\u0026gt;New-\u0026gt;Target). Scroll all the way down in the target types to choose \u0026ldquo;Camera Extension\u0026rdquo;. Title it \u0026ldquo;Extension\u0026rdquo;     Go to the new extension target, select Signing \u0026amp; Capabilities, and remove \u0026ldquo;Hardened Runtime\u0026rdquo;. Under App Sandbox, check \u0026ldquo;Camera\u0026rdquo; and, for debugging purposes only, set \u0026ldquo;User Selected File\u0026rdquo; to \u0026ldquo;Read Only\u0026rdquo;.     See that the extension target has an App Group. Change the extension target app group to $(TeamIdentifierPrefix)com.politepix.OffcutsCam but use your organization identifier instead of mine (com.politepix). Copy this app group name.    Now go back to your app target, Signing \u0026amp; Capabilities, and add the capability \u0026ldquo;App Group\u0026rdquo;. In the new configuration area this adds, click \u0026ldquo;+\u0026rdquo; and add an app group with the identical app group name as the extension ($(TeamIdentifierPrefix)com.politepix.OffcutsCam (but with your organization identifier instead of mine).    Still in your app target, go to Info and add the key Privacy - Camera Usage Description with a description of Camera Extension and while you\u0026rsquo;re here, just to avoid a distracting warning later, add the key App Category and set it to Utilities.     Switching to your extension target, go to Info and add the key Privacy - Camera Usage Description with a description of Camera Extension. There should be a key entitled Privacy - System Extension Usage Description (if there isn\u0026rsquo;t, create it). Add the description Camera Extension to it.     In the extension target, under Info, there should be a key CMIOExtension which is a dictionary. It should contain a key CMIOExtensionMachServiceName. The value of this key should be $(TeamIdentifierPrefix)$(PRODUCT_BUNDLE_IDENTIFIER). Change it to $(TeamIdentifierPrefix)com.politepix.OffcutsCam (but with your organization identifier instead of mine).    Now, theoretically, doing these steps in this order should result in a properly-configured Container App and Embedded System Extension, where the container app is allowed to install its embedded system extension into macOS. To verify this, you can check the following things in your entitlements files:\nApp entitlements should have a System Extension key set to YES. It should have an App Groups array with the first element the string $(TeamIdentifierPrefix)com.politepix.OffcutsCam but with your organization identifier instead of mine. It should have a key Camera set to YES. Here is a screenshot:\n  Extension entitlements should have an identical App Groups array. It should have a key Camera set to YES. Here is a screenshot:\n  The app target\u0026rsquo;s General pane should show the extension as embedded \u0026ldquo;(Embed Without Signing)\u0026rdquo; under Frameworks, Libraries, and Embedded Content.\n  If any of these aren\u0026rsquo;t right, review and see if you set things up correctly. You can compare to my completed version.\nIf this looks good, you can build and run. You should see a \u0026ldquo;Hello, world!\u0026rdquo; app. You can quit it. Go to Xcode-\u0026gt;Product-\u0026gt;Show Build Folder in Finder and find your product OffcutsCam.app, and right-click and choose Show Package Contents the package and verify that you can see the extension inside of it like in this screenshot.\n  OK, stretch your legs for a moment and we\u0026rsquo;ll start configuring the extension and app source.\nReady?\nSource configuration #   Open ExtensionProvider.swift in the editor. This is where the input and output streams for the camera extension are managed. Apple is kind enough to provide a 100% known-working software camera in all fresh ExtensionProviders. I love that they do this. Do a search and replace in this file for every occurrence of SampleCapture and change it to OffcutsCam. Change the occurrence of OffcutsCam (Swift) to just OffcutsCam. This is how we are letting the system and the user know which camera this is.  Inside the CMIOExtensionProvider #  Let\u0026rsquo;s talk briefly about what is going on inside of ExtensionProvider.swift. At the system level, this is the code that the OS extension machinery is going to engage with to provide a camera, starting with initializing its service in Main.swift:\nlet providerSource = ExtensionProviderSource(clientQueue: nil) CMIOExtensionProvider.startService(provider: providerSource.provider) The CMIOExtensionProvider itself has three essential parts:\n The CMIOExtensionProviderSource, a protocol where the conforming class manages the capabilities and properties which affect the entire extension, and where client (video app) connections to the provider (camera extension) are managed, and which creates the object conforming to: CMIOExtensionDeviceSource, a protocol where the conforming class manages the properties which affect the camera device specifically (meaning, the software device or the camera video feed which provides the basis for a creative camera), which configures the object conforming to: the CMIOExtensionStreamSource, a protocol where the conforming class manages the stream mechanics of the camera, such as starting and stopping a stream, and manages the properties of the stream.  This is also the order the objects which conform to these protocols are initialized in, in an installed extension when there is a client that wants to use the camera.\nWhen we replaced the default names with \u0026ldquo;OffcutsCam\u0026rdquo;, we replaced the manufacturer name in CMIOExtensionProviderSource and initialized the deviceSource object with its device name, we set the device model name in its property of CMIOExtensionDeviceSource, and we set the name of the stream where our CMIOExtensionDeviceSource object creates its CMIOExtensionStreamSource object.\nApple\u0026rsquo;s default ExtensionProvider code demonstrates the responsibilities of each of these three parts, and the white line business code (func startStreaming()) can be found in the object conforming to CMIOExtensionDeviceSource before its final buffer is passed to the object conforming to CMIOExtensionStreamSource so it is seen in the client app using the camera extension. The business code that is eventually invoked by the startStream() function of CMIOExtensionStreamSource can get very different in practice, which we\u0026rsquo;ll explore more in part 3 of this series.\nThat\u0026rsquo;s it for ExtensionProvider.swift, for this post (there will be much more ExtensionProvider in the following two posts). But for now, we want to get this default software camera extension fully working so we have a clean canvas to paint on.\nInstallation/deinstallation #  Next, we will use Brad Ford\u0026rsquo;s onscreen sample code for extension install and uninstall from Create camera extensions with Core Media IO , adding it to our container app. First, add import SystemExtensions at the top of the App\u0026rsquo;s file ContentView.swift.\nThen, add the following class to ContentView.swift (my only addition here was more verbose error logging):\nclass SystemExtensionRequestManager: NSObject, ObservableObject { @Published var logText: String = \u0026#34;Installation results here\u0026#34; init(logText: String) { super.init() self.logText = logText } func install() { guard let extensionIdentifier = _extensionBundle().bundleIdentifier else { return } let activationRequest = OSSystemExtensionRequest.activationRequest(forExtensionWithIdentifier: extensionIdentifier, queue: .main) activationRequest.delegate = self OSSystemExtensionManager.shared.submitRequest(activationRequest) } func uninstall() { guard let extensionIdentifier = _extensionBundle().bundleIdentifier else { return } let deactivationRequest = OSSystemExtensionRequest.deactivationRequest(forExtensionWithIdentifier: extensionIdentifier, queue: .main) deactivationRequest.delegate = self OSSystemExtensionManager.shared.submitRequest(deactivationRequest) } func _extensionBundle() -\u0026gt; Bundle { let extensionsDirectoryURL = URL(fileURLWithPath: \u0026#34;Contents/Library/SystemExtensions\u0026#34;, relativeTo: Bundle.main.bundleURL) let extensionURLs: [URL] do { extensionURLs = try FileManager.default.contentsOfDirectory(at: extensionsDirectoryURL, includingPropertiesForKeys: nil, options: .skipsHiddenFiles) } catch { fatalError(\u0026#34;failed to get the contents of \\(extensionsDirectoryURL.absoluteString): \\(error.localizedDescription)\u0026#34;) } guard let extensionURL = extensionURLs.first else { fatalError(\u0026#34;failed to find any system extensions\u0026#34;) } guard let extensionBundle = Bundle(url: extensionURL) else { fatalError(\u0026#34;failed to create a bundle with URL \\(extensionURL.absoluteString)\u0026#34;) } return extensionBundle } } extension SystemExtensionRequestManager: OSSystemExtensionRequestDelegate { public func request(_ request: OSSystemExtensionRequest, actionForReplacingExtension existing: OSSystemExtensionProperties, withExtension ext: OSSystemExtensionProperties) -\u0026gt; OSSystemExtensionRequest.ReplacementAction { logText = \u0026#34;Replacing extension version \\(existing.bundleShortVersion) with \\(ext.bundleShortVersion)\u0026#34; return .replace } public func requestNeedsUserApproval(_ request: OSSystemExtensionRequest) { logText = \u0026#34;Extension needs user approval\u0026#34; } public func request(_ request: OSSystemExtensionRequest, didFinishWithResult result: OSSystemExtensionRequest.Result) { switch result.rawValue { case 0: logText = \u0026#34;\\(request) did finish with success\u0026#34; case 1: logText = \u0026#34;\\(request) Extension did finish with result success but requires reboot\u0026#34; default: logText = \u0026#34;\\(request) Extension did finish with result \\(result)\u0026#34; } } public func request(_ request: OSSystemExtensionRequest, didFailWithError error: Error) { let errorCode = (error as NSError).code var errorString = \u0026#34;\u0026#34; switch errorCode { case 1: errorString = \u0026#34;unknown error\u0026#34; case 2: errorString = \u0026#34;missing entitlement\u0026#34; case 3: errorString = \u0026#34;Container App for Extension has to be in /Applications to install Extension.\u0026#34; case 4: errorString = \u0026#34;extension not found\u0026#34; case 5: errorString = \u0026#34;extension missing identifier\u0026#34; case 6: errorString = \u0026#34;duplicate extension identifer\u0026#34; case 7: errorString = \u0026#34;unknown extension category\u0026#34; case 8: errorString = \u0026#34;code signature invalid\u0026#34; case 9: errorString = \u0026#34;validation failed\u0026#34; case 10: errorString = \u0026#34;forbidden by system policy\u0026#34; case 11: errorString = \u0026#34;request canceled\u0026#34; case 12: errorString = \u0026#34;request superseded\u0026#34; case 13: errorString = \u0026#34;authorization required\u0026#34; default: errorString = \u0026#34;unknown code\u0026#34; } logText = \u0026#34;Extension did fail with error: \\(errorString)\u0026#34; } } in your ContentView, add this variable:\n@ObservedObject var systemExtensionRequestManager: SystemExtensionRequestManager and this view content:\n VStack { Button(\u0026#34;Install\u0026#34;, action: { systemExtensionRequestManager.install() }) Button(\u0026#34;Uninstall\u0026#34;, action: { systemExtensionRequestManager.uninstall() }) } Text(systemExtensionRequestManager.logText) } Change the calls in the container app that load ContentView to load it with its new SystemExtensionRequestManager and let\u0026rsquo;s give the frame a minimum size:\nContentView(systemExtensionRequestManager: SystemExtensionRequestManager(logText: \u0026#34;Starting container app\u0026#34;)) .frame(minWidth: 300, minHeight: 180) Now build and run the app again. You should see a UI with an Install and an Uninstall button.\nClick Install and see that you get an informative error that the container app isn\u0026rsquo;t in /Applications. System extensions have to be installed from a container app in /Applications to be acceptable to the system.\nLet\u0026rsquo;s make things easy on ourselves and set things up so the builds are moved to /Applications so we can install the codesigned extension like an enduser would, but without too much bother.\nLet\u0026rsquo;s edit the scheme for OffcutsCam.app. In the left column under Build, which we can open with the disclosure triangle, add a Post-Action \u0026ldquo;Run Script Action\u0026rdquo; script reading ditto \u0026quot;${CODESIGNING_FOLDER_PATH}\u0026quot; \u0026quot;/Applications/${FULL_PRODUCT_NAME}\u0026quot;. This will copy our app into /Applications once all building and signing is complete.\n  Close the scheme, and build and run. Check /Applications and verify that OffcutsCam.app is in there.\nNext, edit the app scheme a second time, and this time choose Run, and change the Executable. Select \u0026ldquo;Other\u0026rdquo;, and then navigate to your /Applications folder and select your build of OffcutsCam.app that is now there. We\u0026rsquo;ve told the debugger to attach to our copy in /Applications instead of the one in DerivedData.\n  With these steps complete, if you build and run OffcutsCam.app (we never need to run the extension directly), you should be able to click Install in the app UI and do an installation of your extension. The app should log that the extension needs user approval and macOS should show an alert saying \u0026ldquo;System Extension Blocked\u0026rdquo;. That\u0026rsquo;s great! That\u0026rsquo;s how it\u0026rsquo;s supposed to work.\n   Click \u0026ldquo;Open System Settings\u0026rdquo; on that alert (if you clicked \u0026ldquo;OK\u0026rdquo; instead that\u0026rsquo;s fine, go ahead and open the System Settings.app section \u0026ldquo;Security \u0026amp; Privacy\u0026rdquo;). Scroll down until you see the notification System software from application \u0026quot;OffcutsCam\u0026quot; was blocked from loading and click Allow.\n  Authenticate to install. Now the extension should be installed in your system. You can verify this by opening FaceTime. Under the Video menu you should see OffcutsCam as an available camera. If you select it, you should see a black screen with a white line moving up and down it. Congrats! Your first CMIO Camera Extension.\nWe have taken pains to get codesigned extension installation working from the start so that we don\u0026rsquo;t need to debug this area of the project while debugging other complexities later on such as interprocess communication and realtime video processing.\nAnd that brings us to the conclusion of part one of this series: we have created a Container App which can install and uninstall a working Core Media IO Camera System Extension that can be selected in FaceTime, and we have removed one big pain point already, which is testing this behavior with full codesigning while still being able to do a normal build and run.\nMy working version can be seen here, so if you are getting weird results, you can compare them.\nOnce you have things working, if you want to play with the extension provider code (and if you like video code, you probably do), be aware that to see software changes in your extension, you currently have to uninstall the old extension using your container app, then reboot, then build and run your new container app iteration, then install the changed extension. This is also currently the case if you follow the debugging advice for loading a new extension iteration without reversioning it (i.e., turning off SIP and turning on developer mode will not help with this). If I understand correctly, there is no way currently to securely replace the extension process in a single user session in which there has already been an extension install and activation, so a reboot is necessary.\nThis reboot-a-rama is the pain point we are going to remove next, in Core Media IO Camera Extensions part 2 of 3: Creating a useful software CMIO Camera Extension with communication between a configuration app and an extension and painless extension debugging.\nExtro #  ✂ - - Non-code thoughts, please stop reading here for the code-only experience - - ✂\nSince we\u0026rsquo;re combining system extensions, video code, and codesigning, i.e. discussing pain, I thought it might be a good opportunity for some stoic philosophy.\nThere were three big stoics whose work we know about; Seneca, Epictetus and Marcus Aurelius. I\u0026rsquo;m not crazy about Seneca. Marcus Aurelius had some very helpful things to say, but he was an extraordinarily powerful man who has become a sort of aspirational lifestyle stoic: philosopher most likely to be cited in a LinkedIn post. For me, Epictetus is the most interesting. An ancient Greek philosopher, a onetime slave, someone who had to avoid anti-philosopher purges now and again – you\u0026rsquo;re reading a blog post about camera extensions so it\u0026rsquo;s pretty likely that you and I are in the same lucky club of people who have no conception of the perils and problems he experienced, and it\u0026rsquo;s pretty moving to be able to hear his thoughts on how to think about them millenia later.\nAnyway he had this one piece of advice from the Encheiridion (\u0026ldquo;The Handbook\u0026rdquo;) that I think of a lot when I look at our industry. Epictetus is talking about how people jockey to do favors for the powerful, and the way it helps them rise in social or academic status, and that you, philosopher he is addressing, also want this status, but without all the ass-kissing, because it\u0026rsquo;s immoral. And he says (slightly modernized from W.A. Oldfather translation):\n\u0026ldquo;Well, what is the price of a head of lettuce? A dollar perhaps. If, then, somebody gives up his dollar and gets his head of lettuce, while you do not give your dollar, and do not get it, do not imagine that you are worse off than the man who gets his lettuce. For as he has his head of lettuce, so you have your dollar which you have not given away.\u0026rdquo;\nThere are a few points here. First of all, if you really believe this kind of status-seeking is immoral, then you haven\u0026rsquo;t lost anything when you\u0026rsquo;ve held onto your dollar and chosen not to buy the lettuce, and it\u0026rsquo;s important to notice that – it can be difficult to notice because there are a lot of signals from our society which say that status is more important than values. You still have your morality, which should have a very high value to you, so that is actually a good deal. If it still feels like a bad deal, you may have a different opinion of the morality of the situation, or morality in general, than you think you do, or (more likely, in my experience) you may have substituted someone else\u0026rsquo;s opinion for your own.\nSecond, IMO Epictetus is making a little joke by comparing status and morality to lettuce, and the amount of money that lettuce costs, respectively.\nThird, there\u0026rsquo;s an unspoken implication that either can still be chosen differently. Exchanging a dollar for lettuce is easier than exchanging lettuce for a dollar, granted – that is part of his point. But, you could sell your lettuce for a dollar or return it for your original dollar. You could spend your dollar on lettuce. Because of this, his main point is that you shouldn\u0026rsquo;t complain, primarily if you have a dollar but no lettuce, which he thought was the better deal, but perhaps also if you have a lettuce and no dollar. Change the deal to the one you can live with, or accept your decision, but don\u0026rsquo;t lose your limited time envying people who have something you decided that you don\u0026rsquo;t want.\nIn the context of tech, I often think about this advice, not in terms of social status, which isn\u0026rsquo;t a giant influence in the life of software developers (although it is, of course, in the mix), but more in terms of the question of what problems we choose to use our skills to solve and for what compensations, and how we feel about our choices.\n"},{"id":3,"href":"/posts/prototyping-a-stationary-bike-stepper/","title":"Prototyping a stationary bike \"step\" counter with Swift Charts","section":"Posts","content":"Prototyping a stationary bike \u0026ldquo;step\u0026rdquo; counter with Swift Charts #  July 11th, 2022 To the code →\n I was recently chatting with a friend about SwiftUI, and I admitted that I had skipped the awkward first year entirely, but since I had gotten into it, it was reigniting my joy of the platform. \u0026ldquo;Why?\u0026rdquo; he asked. I had to think for a moment. \u0026ldquo;I love prototyping, and SwiftUI removed blockers that I had stopped noticing.\u0026rdquo;\nBack in the age of the dinosaurs, when I was doing a technical proof of concept, I preferred to write Objective-C UIs in code. There\u0026rsquo;s something very consonant, very nimble, about being able to take in an entire logical graph in a single zone of attention: maybe just one file you can see all of. When you\u0026rsquo;re just finding out if something is possible, the tradeoffs of that approach are compensated in flow state.\nSwiftUI feels the same but with more concision, and without the tradeoffs.\nIn the present, with a particular itch to scratch: I have a cheap stationary bike, perfect for my workouts in every respect except its inhumane display interface. And I spend so much time looking at the display and thinking about it! Should I run its signal wires into an MCU and make my own display? Should I use computer vision to consume its data? Is there something like Van Eck phreaking for LEDs?\nOr, can I forget about the display entirely, estimate the bike\u0026rsquo;s stationary km/h using sensors from my phone when it\u0026rsquo;s in the bike\u0026rsquo;s reading material holder, and calculate everything else that\u0026rsquo;s interesting?\nCan I? #  I know from working with a 9-DoF sensor in hardware projects that the odds are decent there\u0026rsquo;s some perceptible movement to use. But while I have theories about which sensors are likely to register the oscillation of a stationary bike (guesses: accelerometer and gyroscope), I wouldn\u0026rsquo;t assume: it\u0026rsquo;s even possible that pedaling the bike produces a field that could be picked up reliably on another sensor. No doubt any data will be small and noisy. I guess I need a prototype!\nThe iPhone\u0026rsquo;s 9-DoF sensors output readings which are arrays of doubles, but like all samples of electromechanical devices, such as the MEMs sensors in an iPhone, these samples are just pointillist representations of real waves. When we talk about extracting meaning from the samples, we use perceptual language like smoothing. If this works, we will most easily see it visually, so we should chart the sensors.\nSwift Charts #  When I first decided to do this post, I checked out all the chart packages for SwiftUI, but none of them quite spoke to me. I put the idea aside for a few days and decided to come back to it later and just pick one, and when I came back to it, WWDC had happened and Apple had released a really snazzy chart framework just for me, thanks Apple! [1]  (German has a highly-pejorative word for taking action purely for the sake of taking action, but it lacks a word for reaping the benefits of inaction.)   var footnotesubscript = document.getElementById(\"footnotesubscript_id_1\"); footnotesubscript.style.cursor = \"pointer\"; footnotesubscript.onclick = function () { var footnote = document.getElementById(\"footnote_id_1\"); if (footnote.style.visibility == \"visible\") { footnote.style.visibility = \"hidden\"; footnote.style.display = \"none\"; footnote.style.position = \"absolute\"; } else { footnote.style.visibility = \"visible\"; footnote.style.position = \"relative\"; footnote.style.display = \"inline\"; } };  This felt like more supporting data for my warm Swift prototyping feelings. It also means you can only run this code from an Xcode 14 beta or later to a real device running an iOS 16 beta or later.\nGoals: #  Write very minimal code for a single view with a chart of every sensor. Don\u0026rsquo;t extensively state- and error-handle; it should be run in portrait mode and it\u0026rsquo;s fine to assume a functioning device. Don\u0026rsquo;t worry about nice user interactions [2]  (I think there is a difference between a UI and a DI or developer interface, and like a wireframe and a design artwork, it can be a pretty good idea to prevent them from being confused with each other)   var footnotesubscript = document.getElementById(\"footnotesubscript_id_2\"); footnotesubscript.style.cursor = \"pointer\"; footnotesubscript.onclick = function () { var footnote = document.getElementById(\"footnote_id_2\"); if (footnote.style.visibility == \"visible\") { footnote.style.visibility = \"hidden\"; footnote.style.display = \"none\"; footnote.style.position = \"absolute\"; } else { footnote.style.visibility = \"visible\"; footnote.style.position = \"relative\"; footnote.style.display = \"inline\"; } }; . Make it possible to:\n see all the sensors, turn off the views of the sensors without relevant data, break out the three sensor axes to inspect them, and, for promising sensors, be able to smooth the wave and count the number of wave peaks (which are equivalent to pedaling \u0026ldquo;strides\u0026rdquo;).  The technical prototype is the question \u0026ldquo;is this possible?\u0026rdquo;; what\u0026rsquo;s inside it is the least complexity that would provide the answer. A product would present a second question, which is \u0026ldquo;can this be generalized?\u0026rdquo;, and the prudent answer is \u0026ldquo;not necessarily\u0026quot; [3]  (although I have some theories)   var footnotesubscript = document.getElementById(\"footnotesubscript_id_3\"); footnotesubscript.style.cursor = \"pointer\"; footnotesubscript.onclick = function () { var footnote = document.getElementById(\"footnote_id_3\"); if (footnote.style.visibility == \"visible\") { footnote.style.visibility = \"hidden\"; footnote.style.display = \"none\"; footnote.style.position = \"absolute\"; } else { footnote.style.visibility = \"visible\"; footnote.style.position = \"relative\"; footnote.style.display = \"inline\"; } }; .\nIn an app, I wouldn\u0026rsquo;t have a model and an interface in the same file. However, another thing I like about SwiftUI is the ability to give someone a single file they can drop into an app and try out just by initializing it in the App struct, with a UI and everything, wow. Trying things out is the name of the game here, so a single file will be my preference in this blog. But I added some fancy comment formatting so it\u0026rsquo;s easy to draw attention to interesting things from inside the file, and indicate when we\u0026rsquo;re in different kinds of modules.\nContentView.swift #  (view on Github)\nhljs.addPlugin(new CopyButtonPlugin()); hljs.highlightLinesAll([[ {start: '11', end: '11', color: 'yellow'}, {start: ' 35', end: '36', color: 'yellow'}, {start: ' 54', end: '54', color: 'yellow'}, {start: ' 77', end: '77', color: 'yellow'}, {start: ' 119', end: '124', color: 'yellow'}, {start: ' 159', end: '159', color: 'yellow'}, {start: ' 230', end: '233', color: 'yellow'}, ],]); // Created by Halle Winkler on July/11/22. Copyright © 2022. All rights reserved. // Requires Xcode 14.x and iOS 16.x, betas included. import Charts import CoreMotion import SwiftUI // MARK: - ContentView /// ContentView is a collection of motion sensor UIs and a method of calling back to the model. struct ContentView { @ObservedObject var manager: MotionManager } extension ContentView: View { var body: some View { VStack { ForEach(manager.sensors, id: \\.sensorName) { sensor in SensorChart(sensor: sensor) { applyFilter, lowPassFilterFactor, quantizeFactor in manager.updateFilteringFor( sensor: sensor, applyFilter: applyFilter, lowPassFilterFactor: lowPassFilterFactor, quantizeFactor: quantizeFactor) } } }.padding([.leading, .trailing], 6) } } // MARK: - SensorChart /// I like to compose SwiftUI interfaces out of many small modules. But, there is a tension when it's a /// small UI overall, and the modules will each have overhead from propagating state, binding and callbacks. struct SensorChart { @State private var chartIsVisible = true @State private var breakOutAxes = false @State private var applyingFilter = false @State private var lowPassFilterFactor: Double = 0.75 @State private var quantizeFactor: Double = 50 var sensor: Sensor let updateFiltering: (Bool, Double, Double) - Void private func toggleFiltering() { applyingFilter.toggle() updateFiltering(applyingFilter, lowPassFilterFactor, quantizeFactor) } } extension SensorChart: View { var body: some View { /// Per-sensor controls: apply filtering to the waveform, hide and show sensor, break out the axes into separate charts. HStack { Text(\"\\(sensor.sensorName)\") .font(.system(size: 12, weight: .semibold, design: .default)) .foregroundColor(chartIsVisible ? .black : .gray) Spacer() Button(action: toggleFiltering) { Image(systemName: applyingFilter ? \"waveform.circle.fill\" : \"waveform.circle\") } .opacity(chartIsVisible ? 1.0 : 0.0) Button(action: { chartIsVisible.toggle() }) { Image(systemName: chartIsVisible ? \"eye.circle.fill\" : \"eye.slash.circle\") } Button(action: { breakOutAxes.toggle() }) { Image(systemName: breakOutAxes ? \"1.circle.fill\" : \"3.circle.fill\") } .opacity(chartIsVisible ? 1.0 : 0.0) } /// Sensor charts, either one chart with three axes, or three charts with one axis. I love how concise Swift Charts can be. if chartIsVisible { if breakOutAxes { ForEach(sensor.axes, id: \\.axisName) { series in // Iterate charts from series Chart { ForEach( Array(series.measurements.enumerated()), id: \\.offset) { index, datum in LineMark( x: .value(\"Count\", index), y: .value(\"Measurement\", datum)) } } Text( \"Axis: \\(series.axisName)\\(applyingFilter ? \"\\t\\tPeaks in window: \\(series.peaks)\" : \"\")\") } .chartXAxis { AxisMarks(values: .automatic(desiredCount: 0)) } } else { Chart { ForEach(sensor.axes, id: \\.axisName) { series in // Iterate series in a chart ForEach( Array(series.measurements.enumerated()), id: \\.offset) { index, datum in LineMark( x: .value(\"Count\", index), y: .value(\"Measurement\", datum)) } .foregroundStyle(by: .value(\"MeasurementName\", series.axisName)) } }.chartXAxis { AxisMarks(values: .automatic(desiredCount: 0)) }.chartYAxis { AxisMarks(values: .automatic(desiredCount: 2)) } } /// in the separate three-axis view, you can set the low-pass filter factor and the quantizing factor if the waveform /// filtering is on, and then once you can see your stationary pedaling reflected in the waveform, you can see how /// many times per time window you're pedaling. With such an inevitably-noisy sensor environment, I already know /// the low-pass filter factor will have to be very high, so I'm starting it at 0.75. /// In the case of my exercise bike, the quantizing factor that delivers very accurate peak-counting results on /// gyroscope axis z is 520, which tells you these readings are really small numbers. if applyingFilter { Slider( value: $lowPassFilterFactor, in: 0.75 ... 1.0, onEditingChanged: { _ in updateFiltering( true, lowPassFilterFactor, quantizeFactor) }) Text(\"Lowpass: \\(String(format: \"%.2f\", lowPassFilterFactor))\") .font(.system(size: 12)) .frame(width: 100, alignment: .trailing) Slider( value: $quantizeFactor, in: 1 ... 600, onEditingChanged: { _ in updateFiltering( true, lowPassFilterFactor, quantizeFactor) }) Text(\"Quantize: \\(Int(quantizeFactor))\") .font(.system(size: 12)) .frame(width: 100, alignment: .trailing) } } Divider() } } // MARK: - MotionManager /// MotionManager is the sensor management module. class MotionManager: ObservableObject { // MARK: Lifecycle init() { self.manager = CMMotionManager() for name in SensorNames .allCases { // self.sensors and func collectReadings(...) use SensorNames to index, if name == .attitude { // so if you change how one creates/derives a sensor index, change them both. sensors.append(ThreeAxisReadings( sensorName: SensorNames.attitude.rawValue, // The one exception to sensor axis naming: axes: [ Axis(axisName: \"Pitch\"), Axis(axisName: \"Roll\"), Axis(axisName: \"Yaw\"), ])) } else { sensors.append(ThreeAxisReadings(sensorName: name.rawValue)) } } self.manager.deviceMotionUpdateInterval = sensorUpdateInterval self.manager.accelerometerUpdateInterval = sensorUpdateInterval self.manager.gyroUpdateInterval = sensorUpdateInterval self.manager.magnetometerUpdateInterval = sensorUpdateInterval self.startDeviceUpdates(manager: manager) } // MARK: Public public func updateFilteringFor( // Manage the callbacks from the UI sensor: ThreeAxisReadings, applyFilter: Bool, lowPassFilterFactor: Double, quantizeFactor: Double) { guard let index = sensors.firstIndex(of: sensor) else { return } DispatchQueue.main.async { self.sensors[index].applyFilter = applyFilter self.sensors[index].lowPassFilterFactor = lowPassFilterFactor self.sensors[index].quantizeFactor = quantizeFactor } } // MARK: Internal struct ThreeAxisReadings: Equatable { var sensorName: String // Usually, these have the same naming: var axes: [Axis] = [Axis(axisName: \"x\"), Axis(axisName: \"y\"), Axis(axisName: \"z\")] var applyFilter: Bool = false var lowPassFilterFactor = 0.75 var quantizeFactor = 1.0 func lowPassFilter(lastReading: Double?, newReading: Double) - Double { guard let lastReading else { return newReading } return self .lowPassFilterFactor * lastReading + (1.0 - self.lowPassFilterFactor) * newReading } } struct Axis: Hashable { var axisName: String var measurements: [Double] = [] var peaks = 0 var updatesSinceLastPeakCount = 0 /// I love sets, like, a lot. Enough that when I first thought \"but what's an *elegant* way to know when it's a /// good time to count the peaks again?\" I thought of a one-liner set intersection, very semantic, very accurate to the /// underlying question of freshness of sensor data, and it made me happy, and I smiled. /// Anyway, a counter does the same thing with a 0s execution time, here's one of those: mutating func shouldCountPeaks() - Bool { // Peaks are only counted once a second updatesSinceLastPeakCount += 1 if updatesSinceLastPeakCount == MotionManager.updatesPerSecond { updatesSinceLastPeakCount = 0 return true } return false } } @Published var sensors: [ThreeAxisReadings] = [] // MARK: Private private enum SensorNames: String, CaseIterable { case attitude = \"Attitude\" case rotationRate = \"Rotation Rate\" case gravity = \"Gravity\" case userAcceleration = \"User Acceleration\" case acceleration = \"Acceleration\" case gyroscope = \"Gyroscope\" case magnetometer = \"Magnetometer\" } private static let updatesPerSecond: Int = 30 private let motionQueue = OperationQueue() // Don't read sensors on main private let secondsToShow = 5 // Time window to observe private let sensorUpdateInterval = 1.0 / Double(updatesPerSecond) private let manager: CMMotionManager private func startDeviceUpdates(manager _: CMMotionManager) { self.manager .startDeviceMotionUpdates(to: motionQueue) { motion, error in self.collectReadings(motion, error) } self.manager .startAccelerometerUpdates(to: motionQueue) { motion, error in self.collectReadings(motion, error) } self.manager.startGyroUpdates(to: motionQueue) { motion, error in self.collectReadings(motion, error) } self.manager .startMagnetometerUpdates(to: motionQueue) { motion, error in self.collectReadings(motion, error) } } private func collectReadings(_ motion: CMLogItem?, _ error: Error?) { DispatchQueue.main.async { // Add new readings on main switch motion { case let motion as CMDeviceMotion: self.appendReadings( [motion.attitude.pitch, motion.attitude.roll, motion.attitude.yaw], to: \u0026self.sensors[SensorNames.attitude.index()]) self.appendReadings( [motion.rotationRate.x, motion.rotationRate.y, motion.rotationRate.z], to: \u0026self.sensors[SensorNames.rotationRate.index()]) self.appendReadings( [motion.gravity.x, motion.gravity.y, motion.gravity.z], to: \u0026self.sensors[SensorNames.gravity.index()]) self.appendReadings( [motion.userAcceleration.x, motion.userAcceleration.y, motion.userAcceleration.z], to: \u0026self.sensors[SensorNames.userAcceleration.index()]) case let motion as CMAccelerometerData: self.appendReadings( [motion.acceleration.x, motion.acceleration.y, motion.acceleration.z], to: \u0026self.sensors[SensorNames.acceleration.index()]) case let motion as CMGyroData: self.appendReadings( [motion.rotationRate.x, motion.rotationRate.y, motion.rotationRate.z], to: \u0026self.sensors[SensorNames.gyroscope.index()]) case let motion as CMMagnetometerData: self.appendReadings( [motion.magneticField.x, motion.magneticField.y, motion.magneticField.z], to: \u0026self.sensors[SensorNames.magnetometer.index()]) default: print(error != nil ? \"Error: \\(String(describing: error))\" : \"Unknown device\") } } } private func appendReadings( _ newReadings: [Double], to threeAxisReadings: inout ThreeAxisReadings) { for index in 0 ..= Int(1.0 / self .sensorUpdateInterval * Double(self.secondsToShow)) { axis.measurements .removeFirst() // trim old data to keep our moving window representing secondsToShow } threeAxisReadings.axes[index] = axis } } private func countPeaks( in readings: [Double], quantizeFactor: Double) - Int { // Count local maxima let quantizedreadings = readings.map { Int($0 * quantizeFactor) } // Quantize into small Ints (instead of extremely small Doubles) to remove detail from little component waves var ascendingWave = true var numberOfPeaks = 0 var lastReading = 0 for reading in quantizedreadings { if ascendingWave == true, lastReading  reading { // If we were going up but it stopped being true, numberOfPeaks += 1 // we just passed a peak, ascendingWave = false // and we're going down. } else if lastReading Self.AllCases .Index { // Force-unwrap of index of enum case in CaseIterable always succeeds. return Self.allCases.firstIndex(of: self)! } } typealias Sensor = MotionManager.ThreeAxisReadings  Here is a video of my using the completed prototype. It works; you can see that I turn off the sensors which aren\u0026rsquo;t reacting to my pedaling at all, then check the three sensors which do react. I turn off the first two because I don\u0026rsquo;t think the waveform of the bike oscillation is very clear. But in the last one, I can see it quite clearly on axis z. So, I turn on the low-pass filter, turn it up almost all the way, and set the quantizing to a very large number. It accurately counts how often I pedal per window.\nThere should have been a video here but your browser does not seem to support it.  Extro #  ✂ - - - Non-code thoughts, please stop reading here for the code-only experience - - - ✂\nI haven\u0026rsquo;t been a fan of what I would describe as the Corona Aesthetic in independent filmmaking (2-4 people deal with something ambiguous and hastily-written in a room, or a forest, or a disused building, or online). Not because I\u0026rsquo;m annoyed that no one made pre-Corona-style films for my entertainment during a worldwide crisis; because I believe it was a profoundly creatively-impaired period and it\u0026rsquo;s a collective good to own it.\nEveryone was reckoning with existential fears, and their limbic systems were loudly lit up in unfamiliar, uninteresting ways, and when I watch these films, I feel like I\u0026rsquo;m watching a guy lose the battle to acknowledge to himself that this just isn\u0026rsquo;t going to be the year and he should tend to his own garden, instead of making a public demonstration of his ability to power through his stuck brain problems. As the audience, I feel dropped into the role of validator. Honestly, I feel like feature narrative film is simply too large, rigid, and wasteful a medium for the circumstances.\nBut, oh, the non-narrative short film. Amyl and the Sniffers are a phenomenal punk band out of Melbourne, and John Angus Stewart is a filmmaker from the same town who has made a bunch of their gorgeous videos. For me, this trio of shorts is the best audiovisual summation of the pandemic: what it\u0026rsquo;s like being a little too intense and in an isolated feedback loop with your own energies, longing for connection:\n Guided by Angels Hertz Security  "},{"id":4,"href":"/about/","title":"About | Contact","section":"Introduction","content":"Contact #  You can send me a note here.\nAbout me #  I\u0026rsquo;m Halle (she/her). I\u0026rsquo;ve lived in Berlin for decades but I\u0026rsquo;m originally from New York City. I grew up playing in bands, including one of the first riot grrl bands in NYC, writing music and studying music composition, doing audio engineering, and experimenting in software audio synthesis back when it was done in NeXTSTEP (although I wrote my first audio code ever when I was 10, on a VIC-20). I etched the PCB for my first guitar distortion pedal. It didn\u0026rsquo;t work! Later on, I got a lot better at hardware.\nI spent early adulthood inline-skating around NYC helping companies deal with the first year of the web, and art directed and coded several early-web firsts: the first commercial fashion website, and the first commercial record label website, after which I moved out west and was a double-digit employee of the US\u0026rsquo; first cable broadband company, as a senior technical designer for their daily-published media site; to the best of my knowledge the first site which served inline video as its standard content.\nI had a full and fulfilling career as a graphic designer who could code, and then starting in 2004 I began getting back to my NeXTSTEP programmatic synthesis roots and writing software for Apple platforms, starting with some Final Cut Pro plugins for video post-processing. In 2008 I worked through the CS courses that Stanford University made free and public, started writing apps for the iPhone from year one, and since then have worked on many interesting, beautiful, and challenging things. I really liked Objective-C, I really like Swift, and I guess I\u0026rsquo;m comfortable with change!\nAbout The Offcuts #  Think about the shop: the wood shop, the metal shop, the sewing shop, the print shop. Now think about that box of offcuts: the beautiful, strange, oddly-shaped extra parts that were left over when the other thing was produced. They\u0026rsquo;re free; you can make anything you want with them, if you have an idea.\n"},{"id":5,"href":"/privacypolicy/","title":"Privacy Policy","section":"Introduction","content":"Privacy #  The policy is that you have privacy. This site has no cookies or accounts so there is no way to persist any information about you here. I don\u0026rsquo;t have access to the visitor logs. Your personally identifiable information is not collected by this blog or by me. The webhost is Github Pages, which means it is possible that you have a different privacy relationship with Github, so you can also check out their privacy policy.\nConsequent to the fact that this is a personal blog with no way of storing information about you, there is no DPO because there would be nothing for them to do.\nThe responsible party under the TKG is Halle Winkler, and you can contact me here.\n"},{"id":6,"href":"/workwithhalle/","title":"Work with Halle","section":"Introduction","content":"Work with Halle #  I sometimes have availability for contracts. My preference is a contract of 3-6 months, but I\u0026rsquo;m open-minded, so please feel free to get in touch.\n"}]